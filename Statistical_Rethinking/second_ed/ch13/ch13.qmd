---
title: "Rethinking - Chapter 13"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true

library(rethinking)
library(dagitty)
library(tidyverse)
```

## 13.0

**Multilevel models**

- Improved estimates for repeated sampling
- Improved estimates for imbalance in sampling
- Estimates of variation
  - How does the mean of groups vary
- Avoid averaging, retain variation
  - Instead of data transformations that create new features based on averages, model the variation explicitly.

Multilevel models should always be the default. 

## 13.1 - Example: Multilevel tadpoles

Reed frog mortality data:

```{r}
data('reedfrogs')
d <- reedfrogs
str(d)

```

Each row in the data is a different tank. We are interested in the number that `surv` in each based on the `denisty` ($N$). We'll start with the simple naive model from earlier chapters:

\begin{align}
S_i &\sim \text{Binomial}(p_i, N_i) \\
\text{logit}(p_i) &= \alpha_{\text{TANK}[i]} \\ 
\alpha_j &\sim \text{Normal}(0, 1.5)
\end{align}

```{r}
#| output: false

d$tank <- 1:nrow(d)
dat <- list(S=d$surv, N=d$density, tank=d$tank)

m13.1 <- ulam(alist(
  S ~ dbinom(N, p),
  logit(p) <- a[tank],
  a[tank] ~ dnorm(0, 1.5)
), data=dat, chains=4, log_lik=T)
```


```{r}
precis(m13.1, depth=2) |> head()
```

Let's now look at the multilevel model:

\begin{align}
S_i &\sim \text{Binomial}(p_i, N_i) \\
\text{logit}(p_i) &= \alpha_{\text{TANK}[i]} \\ 
\alpha_j &\sim \text{Normal}(\bar{\alpha}, \sigma) \\ 
\bar{\alpha} &\sim \text{Normal}(0, 1.5) \\ 
\sigma &\sim \text{Exponential}(1)
\end{align}

And fit it with `ulam`:

```{r}
#| output: false

m13.2 <- ulam(alist(
  S ~ dbinom(N, p),
  logit(p) <- a[tank],
  a[tank] ~ dnorm(a_bar, sigma),
  a_bar ~ dnorm(0,1.5),
  sigma ~ dexp(1)
), data=dat, chains=4, log_lik=T)
```

```{r}
compare(m13.1, m13.2)
```

Notice that for both models, they have smaller number of effective parameters than they do in the real model. This is because both models employ some sort of regularizing prior. *But*, the multilevel model has an even stronger regularizing prior even though it has two more real parameters. This is of course because it learned a stronger prior through the data.

Let's now plot the effect of using the multilevel model on the parameters.

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

post <- extract.samples(m13.2)
d$propsurv.est <- logistic(apply(post$a, 2, mean))

plot(d$propsurv, ylim=c(0,1), pch=16, xaxt='n', xlab='tank', 
     ylab='proportion survival', col=rangi2)
c(1, 16, 32, 48) %>% {axis(1, at=., label=.)}

points(d$propsurv.est)

post$a_bar |> inv_logit() |> mean() |> abline(h=_, lty=2)

c(16.5, 32.5) |> abline(v=_, lwd=0.5)
text(8,0,'small tanks')
text(8+16,0,'medium tanks')
text(8+32,0,'large tanks')
title('Blue=Data Prop, Clear=Model Est')
```

Let's take a closer look at the posterior distribution for $\bar \alpha$ and $\sigma$ by looking at the Normal distribution they produce:

```{r}
plot(NULL, xlim=c(-3,7), ylim=c(0,0.35), xlab='log-odds survival', ylab='Density')
for (i in 1:100){
  curve(dnorm(x, post$a_bar[i], post$sigma[i]), add=T, col=col.alpha('black', 0.2))
}
```
Let's sample some "tanks" for the inferred population distribution of tanks:

```{r}
sim_tanks <- rnorm(8e3, post$a_bar, post$sigma) # post will be recycled
sim_tanks |> inv_logit() |> dens(lwd=2, adj=0.3)
```

We talked in the previous chapter about how the beta-binomial or gamma-Poisson accounted for over-dispersion in our data. They did this by adding an extra variance parameter (which was really just marginalized out intercepts). The multilevel model achieves over-dispersion correction as well since we get one observation per group.

One thing to note - we have been using the exponential prior almost exclusively for $\sigma$ parameters. This is okay usually, but can be problematic when $\sigma$ *can* but shouldn't take larger values. This is because the exponential distribution has long tails which give way to more plausible large variances. You can always mitigate this by using a Half-Normal prior with of course has much thinner tails.

## 13.2 - Varying Effects and the underfitting/overfitting trade-off

One of the perks of using varying effects is that we are less underfit than the grand mean (complete pooling), and less overfit than the no-pooling estimates (one intercept per tank). This is especially true for when a tank has a low amount of data and it overfits quite aggressively.

Let's now simulate some data, but instead of tanks, we'll call them ponds. This will make our mission to predict future ponds survival rate more realistic. Recall the data generating process we modeled before:


\begin{align}
S_i &\sim \text{Binomial}(p_i, N_i) \\
\text{logit}(p_i) &= \alpha_{\text{TANK}[i]} \\ 
\alpha_j &\sim \text{Normal}(\bar{\alpha}, \sigma) \\ 
\bar{\alpha} &\sim \text{Normal}(0, 1.5) \\ 
\sigma &\sim \text{Exponential}(1)
\end{align}

To simulate from this, we need to define $\bar \alpha$, $\sigma$, and a vector $\alpha$ for the true pond effects. We'll also need to assign $N_i$ for each pond.

Notice that the priors are part of the model when we estimate, but not when we simulate. This is because priors are epistemological, not ontological. That is, priors are from incomplete knowledge of the process, not the inherit random of the process itself. Similar to a coin flip, we know there is some uncertainty in the weightedness of the coin - governing the probability that it comes up heads. But there is also uncertainty from the inherit randomness of the system that we won't be able to pin down. The latter type of randomness is that from which we sample from. For simulating, we would lock down the probability of the coin.


```{r}
a_bar <- 1.4
sigma <- 1.5
nponds <- 60
Ni <- c(5,10,25,35) |> rep(each=15) |> as.integer()

set.seed(5005)
a_pond <- rnorm(nponds, a_bar, sigma)
dsim <- data.frame(pond=seq_along(a_pond), Ni, true_a=a_pond)
```

Recall that we can convert to log-odds to probabilities with the `logistic` function:

```{r}
#| output: false

dsim$Si <- rbinom(nponds, prob=logistic(dsim$true_a), size=dsim$Ni)

# no pooling estimates
dsim$p_nopool <- dsim$Si / dsim$Ni

# partial pooling estimates
m13.3 <- ulam(alist(
  Si ~ dbinom(Ni, p),
  logit(p) <- a[pond],
  a[pond] ~ dnorm(a_bar, sigma),
  a_bar ~ dnorm(0,1.5),
  sigma ~ dexp(1)
), data=dsim, chains=4, log_lik=T)
```

```{r}
post <- extract.samples(m13.3)
dsim$p_partpool <- post$a |> inv_logit() |> apply(2, mean)
```

Now compute the error between the two estimates:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
dsim$p_true <- inv_logit(dsim$true_a)
nopool_error <- abs(dsim$p_nopool - dsim$p_true)
partpool_error <- abs(dsim$p_partpool - dsim$p_true)

err_grp <- dsim %>% mutate(nopool_error, partpool_error) %>% 
  group_by(Ni) %>% summarise(m_nopool=mean(nopool_error), m_part=mean(partpool_error))

plot(1:60, nopool_error, xlab='pond', ylab='absolute error', col=rangi2, pch=16)
points(1:60, partpool_error)
for (i in unique(dsim$Ni)){
  end_pt <- max(which(dsim$Ni == i))
  abline(v=end_pt + 0.5)
  lines(which(dsim$Ni == i), err_grp %>% filter(Ni==i) %>% pull(m_nopool) %>% rep(15), lwd=2, col=rangi2)
  lines(which(dsim$Ni == i), err_grp %>% filter(Ni==i) %>% pull(m_part) %>% rep(15), lwd=2, lty=2)
}

```

Notice the error is much larger for smaller ponds. The blue line is the error for no pooling estimates. You'll notice that partial pooling does better on average.

Partial pooling might not always be the answer. If outlier points are suspected to be possible, then partial pooling might regularize those points to strongly.

## 13.3 - More than one type of cluster

