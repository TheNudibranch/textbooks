---
title: "Rethinking - Chapter 4"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true
library(rethinking)
```

## 4.0
- Ptolemaic strategy is the same as a fourier series
  - Was able to decompose the orbits into a series of sine and cosine waves
  - Geocentric model can approximate any orbital pattern by applying enough epicycles

## 4.1 Why normal distributions are normal
- Central limit theorem
  - The average of multiple draws from the same distribution will be normal (the sample of repeated draws and averages)
- Footnote 65: famous textbook that says even when we prove why the central limit theorem occurs, it is not intuitive
- Example where we sum 16 draws from a $U(-1,1)$ dist.    
  - $V[U(-1,1)] = \frac{4}{12}$
  - $V[X=\sum^{16}_{i=1}U(-1,1)] = 16 V[U(-1,1)]$

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

pos <- replicate(1e3, sum(runif(16,-1,1)))
dens(pos, lwd=2, col='darkblue')
curve(dnorm(x, 0, sqrt(16*(4/12))), -10,10, add=T, col=2, lwd=2)
```

- Footnote 66 talks about proving the CLT using fourier series
- CLT works as intended only when the original distribution has finite variance
  - Going to see more about this when talking about $\hat{k}$ in LOO-CV


- CLT also works nice when we use products of small increases
  - This is becuase all the small multiplications can be approximated with additions
    - $1.1 \times 1.1 = (1 + 0.1)(1 + 0.1) = 1 + 0.2 + 0.2 + 0.01 \approx 1.2$
  - If we use 1.5 instead of 1.1, it does not look so normal
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

growth <- replicate(1e3, prod(runif(12,1,1.1)))
dens(growth, col='darkblue', lwd=2)
```

- We also get Gaussian back when we log multiplicative effects

## 4.3 A Gausian model of height

```{r}
data(Howell1)
d <- Howell1
precis(d)
```

- Remove all non-adults and plot height
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

d2 <- d[d$age >= 18, ]
hist(d2$height)
```

- Prior predictive checks for model:
\begin{align}
h_i &\sim \text{Normal}(\mu, \sigma)\\
\mu &\sim \text{Normal}(178, 20) \\
\sigma &\sim \text{Uniform}(0,50)
\end{align}

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

n <- 1e4
mu <- rnorm(n, 178, 20)
sig <- runif(n,0,50)
prior_pred <- rnorm(n,mu,sig)
par(mfrow=c(2,2))
dens(mu, lwd=2, col='darkblue', main='mu')
dens(sig, lwd=2, col='darkblue', main='sigma')
dens(prior_pred, lwd=2, col='darkblue', main='prior pred.')
par(mfrow=c(1,1))
```

- Grid Approximation of this model
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

post <- expand.grid(mu=seq(150, 160, length.out=100), 
                    sig=seq(7, 9, length.out=100))
post$LL <- apply(post, 1, \(x) {
  sum(dnorm(d2$height, x[1], x[2], log=T)) +  # likelihood
    dnorm(x[1], 178, 20, log=T) + dunif(x[2], 0, 50, log=T) #priors
})

# To not get a vector of all zeros (just doing exp(...)), we need to scale by the max
post$prob <- exp(post$LL - max(post$LL))

par(mfrow=c(1,2))
contour_xyz(post$mu, post$sig, post$prob)
image_xyz(post$mu, post$sig, post$prob)
par(mfrow=c(1,1))
```

- Let's sample the posterior predictive
  - We want to sample the **combination** of parameters (joint posterior). Not each param. independently
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

samp <- sample(nrow(post), replace=T, prob=post$prob)
par(mfrow=c(2,2))
with(post[samp,], {
  plot(mu, sig, pch=16, asp=1, cex=0.5, col=col.alpha(rangi2,0.1))
  dens(mu, adj=0.8)
  dens(sig)
})
par(mfrow=c(1,1))
```

- Everything looks approximately normal, but what happens if we use only twenty data points:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
#| code-fold: true

d3 <- sample(d2$height, 20)
post <- expand.grid(mu=seq(150, 160, length.out=100), 
                    sig=seq(7, 9, length.out=100))
post$LL <- apply(post, 1, \(x) {
  sum(dnorm(d3, x[1], x[2], log=T)) +  # likelihood
    dnorm(x[1], 178, 20, log=T) + dunif(x[2], 0, 50, log=T) #priors
})

# To not get a vector of all zeros (just doing exp(...)), we need to scale by the max
post$prob <- exp(post$LL - max(post$LL))

samp <- sample(nrow(post), replace=T, prob=post$prob)
par(mfrow=c(2,2))
with(post[samp,], {
  plot(mu, sig, pch=16, asp=1, cex=0.5, col=col.alpha(rangi2,0.1))
  dens(mu, adj=0.8)
  dens(sig)
})
par(mfrow=c(1,1))
```

- Quadratic approximation time!
```{r}
# `alist` does not evaluate the code inside, `list` does
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

# can also provide starting values to start the climb at
m4.1 <- quap(flist, data=d2) 
precis(m4.1)
```
- Because the quadratic approximation is just a guassian approximation, we are left with a mean and variance of said approximation

```{r}
vcov(m4.1)
```
- Sampling the posterior is of course the same as sampling from a multivariate normal
```{r}
post <- extract.samples(m4.1, n=1e4)
# Same as:
post1 <- MASS::mvrnorm(n=1e4, mu=coef(m4.1), Sigma=vcov(m4.1))
head(post)
```

## 4.4 Linear Prediction
- How does height covary with weight
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

d <- Howell1; d2 <- d[d$age >= 18,]
with(d2, plot(weight, height))
```

Introduce a new model for height:

\begin{align}
h_i &\sim \text{Normal}(\mu_i, \sigma)\\
\mu_i &= \alpha + \beta (x_i - \bar{x})\\
\mu &\sim \text{Normal}(178, 20) \\
\sigma &\sim \text{Uniform}(0,50)
\end{align}

- We want to parameters to not just be names that persist across contexts (i.e. **intercept** and **slope**), but to actually mean something
  - Here $\alpha$ is the expected height when weight is at its average value
  - Here $\beta$ is the expected change in height for one unit change in weight
- There is nothing special about the linear relationship, we are free to do whatever you want!

Let's simulate  some prior predictive checks for the model. We'll also see what happens if we let $\beta \sim \text{Log-Normal}(0,1)$ since we know that $\beta$ should be constrained to be positive.
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

par(mfrow=c(1,2))
N <- 100 # Number of lines
a <- rnorm(N, 178, 20)
b <- rnorm(N, 0, 10)
plot(NULL, xlim=range(d2$weight), ylim=c(-100, 400), xlab='weight', ylab='height', main='b ~ Normal(0,10)')
abline(h=0, lty=2)
abline(h=272, lty=1, lwd=0.5)

xbar <- mean(d2$weight)
for (i in 1:N) curve(
  a[i] + b[i]*(x - xbar),
  from=min(d2$weight), to=max(d2$weight), add=T, col=col.alpha('black', 0.2)
)

b <- rlnorm(N, 0, 1)
plot(NULL, xlim=range(d2$weight), ylim=c(-100, 400), xlab='weight', ylab='height', main='b ~ Normal(0,10)')
abline(h=0, lty=2)
abline(h=272, lty=1, lwd=0.5) # World's tallest person is 272

for (i in 1:N) curve(
  a[i] + b[i]*(x - xbar),
  from=min(d2$weight), to=max(d2$weight), add=T, col=col.alpha('black', 0.2)
)
par(mfrow=c(1,1))
```

Let's now fit this model using `quap`:
```{r}
m4.3 <- quap(alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b*(weight - xbar),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0,1),
  sigma ~ dunif(0,50)
), data=d2)

precis(m4.3)
vcov(m4.3) |> round(3)
```
The lack of covariance here is actually do the centering. Refer to exercise at the end of the chapter.

Let's now build the posterior predictive.
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

with(d2, plot(weight, height, col=rangi2, pch=16))
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)
curve(a_map + b_map*(x-xbar), add=T, lwd=2)
```

That is just the mean. How is the cloud of equally likely regression lines distributed around that mean? Let's vary the data fed into the model to find out:
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

plot_post <- function(n){
  new_d <- d2[sample(nrow(d2), n), ]
  with(new_d, plot(weight, height, col=rangi2, pch=16, xlim=range(d2$weight), ylim=range(d2$height), main=paste0('N=',n)))
  m <- quap(alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data=new_d)
  
  post <- extract.samples(m, 20)
  
  for (i in 1:20) curve(post$a[i] + post$b[i]*(x-xbar), add=T, col=col.alpha('black', 0.3))
}

par(mfrow=c(2,2))
for(j in c(10, 50, 150, 352)) plot_post(j)
par(mfrow=c(1,1))
```

We can use the `link` function, which is basically `rstanarm`'s `posterior_linpred`. We can also feed new data into it to get:
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

weight_seq <- seq(25,70)
mu <- link(m4.3, data=data.frame(weight=weight_seq))

par(mfrow=c(1,2))
with(d2, plot(weight, height, type='n'))
for(i in 1:100) points(weight_seq, mu[i,], pch=16, col=col.alpha(rangi2,0.1))

with(d2, plot(weight, height, col=rangi2))
lines(weight_seq, colMeans(mu))
shade(apply(mu, 2, PI, prob=0.89), weight_seq)

par(mfrow=c(1,1))
```

Great, we have the posterior predictive distribution for the mean. But we need the *prediction interval*. This is what is actually "reproducing" the data. Everyone is always so concerned with the mean, but if we only look at the mean, we will fail. We can use the `sim` function to simulate from the Gaussian distribution of height:
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

sim_height <- sim(m4.3, data=data.frame(weight=weight_seq))

plot(height ~ weight, data=d2, col=col.alpha(rangi2,0.5))
lines(weight_seq, colMeans(mu))
shade(apply(mu, 2, PI, 0.89), weight_seq)
shade(apply(sim_height, 2, PI, 0.89), weight_seq)
```
There are two types of uncertainty we have encountered so far: parameter and sampling. The parameter uncertainty is from the model and how inference is done. The sampling (Gaussian noise here) is how the data is "replicated". If we add more parameters to the model and better model the actual process that generates the models, our sampling uncertainty will decrease. 

## 4.5 Curves from Lines
### Polynomial Regression
We will start will *polynomial regression*. Two key differences here is that the $x$'s are now standardized, and we are going to consider the full dataset. Not just the adults.

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

d <- Howell1
d$weight_s <- (d$weight - mean(d$weight))/sd(d$weight)
d$weight_s2 <- d$weight_s^2

plot(height ~ weight, data=d, col=rangi2)
```

The model is defined below: 

\begin{align}
h_i &\sim \text{Normal}(\mu_i, \sigma)\\
\mu_i &= \alpha + \beta_1 (x_i) + \beta_2 (x_i)\\
\beta_1 &\sim \text{Log-Normal}(0,1) \\
\beta_2 &\sim \text{Normal}(0, 1) \\
\alpha &\sim \text{Normal}(178, 20) \\
\sigma &\sim \text{Uniform}(0,50)
\end{align}

We fit as usual:

```{r}
m4.5 <- quap(alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b1*weight_s + b2*weight_s2,
  a ~ dnorm(178, 20),
  b1 ~ dlnorm(0,1),
  b2 ~ dnorm(0,1),
  sigma ~ dunif(0,50)
), data=d)

precis(m4.5)
```
The parameters are no longer interpretable in the same way. We still might want to interpret $\alpha$ as the average value of height when the data takes the average, but this is no longer true. Look at footnote 76 for more context.

For completeness, let's graph the posterior predictive of the mean and response.
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

ws_seq <- seq(-3,2,length.out=500)
new_d <- list(weight_s=ws_seq, weight_s2=ws_seq^2)
mu <- link(m4.5, data=new_d)
height_sim <- sim(m4.5, data=new_d)
plot(height ~ weight, data=d, col=col.alpha(rangi2, 0.3), pch=16)
mu |> apply(2, PI, .89) |> shade(ws_seq*sd(d$weight) + mean(d$weight))
height_sim |> apply(2, PI, .89) |> shade(ws_seq*sd(d$weight) + mean(d$weight))

```


We also are not equipped with any way to say whether one model is better than another. Is the linear better than the quadratic? What about a cubic? Answers will come in chapter 7.

### Splines
For our splines we are going to be using cherry blossom data
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

data(cherry_blossoms)
d <- cherry_blossoms

plot(doy ~ year, data=d, col=rangi2)
```

Let's choose 15 knots for our spline. We are also going to let R create a basis for a cubic spline (degree 3) with the 15 knots.
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6


d2 <- d[!is.na(d$doy), ]
num_knots <- 15
knot_list <- quantile(d2$year, probs=seq(0,1,length.out=num_knots))

library(splines)
B <- bs(
  d2$year, knots=knot_list[-c(1,num_knots)], degree=3,intercept=T
)

plot(NULL, xlim=range(d2$year), ylim=c(0,1), xlab='year', ylab='basis')
for(i in 1:ncol(B)) lines(d2$year, B[,i])
```

The model now takes the following form:

\begin{align}
D_i &\sim \text{Normal}(\mu_i, \sigma)\\
\mu_i &= \alpha + \sum_{k=1}^K w_k B_{k,i}\\
\alpha &\sim \text{Normal}(100, 10) \\
w_j &\sim \text{Normal}(0,10)\\
\sigma &\sim \text{Exponential}(1)
\end{align}

We will use matrix multiplication to fit this in `quap`

```{r}
m4.7 <- quap(alist(
  D ~ dnorm(mu, sigma),
  mu <- a + B %*% w,
  a ~ dnorm(100, 10),
  w ~ dnorm(0,10),
  sigma ~ dexp(1)
), 
data = list(D=d2$doy, B=B),
start = list(w=rep(0,ncol(B)))
)
```

Let's finally plot the posterior:
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

post <- extract.samples(m4.7)
w <- apply(post$w, 2, mean)

par(mfrow=c(2,1))
plot(NULL, xlim=range(d2$year), ylim=c(-6,6), xlab='year', ylab='basis * weight')
for(i in 1:ncol(B)) lines(d2$year, w[i]*B[,i])

mu <- link(m4.7)
plot(d2$year, d2$doy, pch=16, col=col.alpha(rangi2,0.3))
mu |> apply(2,PI,.89) |> shade(d2$year, col=col.alpha('black', 0.5))
par(mfrow=c(1,1))
```

## Problems
### M1
For the model:

\begin{align}
y_i &\sim \text{Normal}(\mu, \sigma)\\
\mu &\sim \text{Normal}(0,10)\\
\sigma &\sim \text{Exponential}(1)
\end{align}

Generate values for the prior predictive

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

n <- 1e4
mu <- rnorm(n,0,10)
sig <- rexp(n,1)
y_prior_pred <- rnorm(n,mu,sig)
hist(y_prior_pred)
```

### M.2
Translate the above model to `quap` notation:

```R
y ~ dnorm(mu, sigma),
mu ~ dnorm(0,10),
sigma ~ dexp(1)
```

### M.3
Translate the following `quap` model:

```R
y ~ dnorm(mu, sigma),
mu <- a + b*x,
a ~ dnorm(0, 10),
b ~ dunif(0, 1),
sigma ~ dexp(1)
```

into math notation:

\begin{align}
y_i &\sim \text{Normal}(\mu, \sigma)\\
\mu &= \alpha + \beta x_i\\
\alpha &\sim \text{Normal}(0,10)\\
\beta &\sim \text{U}(0,1)\\
\sigma &\sim \text{Exponential}(1)
\end{align}

### M.4
Let $y$ be height and $x$ be the year. I'm going to set the average at 173cm, and say that on average kids grow 4 inches a year (10 cm). I plead ignorance on $\sigma$, but exponential is a good choice.:

\begin{align}
y_i &\sim \text{Normal}(\mu, \sigma)\\
\mu &= \alpha + \beta x_i\\
\alpha &\sim \text{Normal}(173,20)\\
\beta &\sim \text{Normal}(10,4)\\
\sigma &\sim \text{Exponential}(1)
\end{align}


### M.5
Oh but heck, you're right! Kids can't shrink unless you're Rick Moranis! I'll change it so the mean of the Log-Normal is still 10cm though. 

\begin{align}
y_i &\sim \text{Normal}(\mu, \sigma)\\
\mu &= \alpha + \beta x_i\\
\alpha &\sim \text{Normal}(173,20)\\
\beta &\sim \text{Log-Normal}(1.8,1)\\
\sigma &\sim \text{Exponential}(1)
\end{align}

### M.6
Oh good call, the variance is never more than 64, which means the standard deviation is never more than 8. Let's find the mean value of the exponential that will place no more than 0.01 mass of the probability above 8. We can do this by inverting the CDF:

```{r}
lam <- -log(1-0.99)/8
qexp(0.99, rate=lam)
lam |> round(2)
```
Let's sub it in to get:
\begin{align}
y_i &\sim \text{Normal}(\mu, \sigma)\\
\mu &= \alpha + \beta x_i\\
\alpha &\sim \text{Normal}(173,20)\\
\beta &\sim \text{Log-Normal}(1.8,1)\\
\sigma &\sim \text{Exponential}(0.58)
\end{align}

### M.7
Refit `m4.3`, but omit the mean weight. 

```{r}
data(Howell1)
d <- Howell1; d2 <- d[d$age >= 18,]
xbar <- mean(d2$weight)

m4.3 <- quap(alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b*(weight - xbar),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0,1),
  sigma ~ dunif(0,50)
), data=d2)

m4.3_1 <- quap(alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b*(weight),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0,1),
  sigma ~ dunif(0,50)
), data=d2)

vcov(m4.3)
vcov(m4.3_1)
```
Woah, what the heck! Looks like the variance for $\beta$ and $\sigma$ are the same but $\alpha$ isn't. Also, we have some covariances now! Let's look at the posterior predictive:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

weight_seq <- seq(25,70)
mu <- link(m4.3, data=data.frame(weight=weight_seq))

par(mfrow=c(2,2))
with(d2, plot(weight, height, type='n'))
for(i in 1:100) points(weight_seq, mu[i,], pch=16, col=col.alpha(rangi2,0.1))

with(d2, plot(weight, height, col=rangi2))
lines(weight_seq, colMeans(mu))
shade(apply(mu, 2, PI, prob=0.89), weight_seq)

mu <- link(m4.3_1, data=data.frame(weight=weight_seq))

with(d2, plot(weight, height, type='n'))
for(i in 1:100) points(weight_seq, mu[i,], pch=16, col=col.alpha(rangi2,0.1))

with(d2, plot(weight, height, col=rangi2))
lines(weight_seq, colMeans(mu))
shade(apply(mu, 2, PI, prob=0.89), weight_seq)
```

Looks the same to me! Must be that the new terms in our covariance matrix are able to capture something extra...

### H.1
Use `m4.3` from above to make predictions.

```{r}
new_weights <- c(46.95, 43.72, 64.78, 32.59, 54.63)
mu <- link(m4.3, data=data.frame(weight=new_weights))
# Expected heights:
colMeans(mu)

# Intervals
apply(mu, 2, PI, 0.89)
```
### H.2
Use ages below 18. 
#### a
```{r}
d3 <- Howell1[Howell1$age < 18, ]
xbar <- d3$weight |> mean()
m_h2 <- quap(alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b*(weight - xbar),
  a ~ dnorm(178, 20),
  b ~ dnorm(0,1),
  sigma ~ dunif(0,50)
), data=d3)

precis(m_h2)
```

For a child at the average weight, we would expect the average height to be 108.38 cm ($\alpha$). A 10 unit increase in weight would result in a 27.1 cm increase in height ($\beta$).

#### b
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

weight_seq <- seq(3, 45, length.out=1e3)
mu <- link(m_h2, data=data.frame(weight=weight_seq))


sim_height <- sim(m_h2, data=data.frame(weight=weight_seq))

plot(height ~ weight, data=d3, col=col.alpha(rangi2,0.5))
lines(weight_seq, colMeans(mu))
shade(apply(mu, 2, PI, 0.89), weight_seq)
shade(apply(sim_height, 2, PI, 0.89), weight_seq)
```

#### c
The model is doing poor at the extremes. A spline or quadratic term could fix this.

### H.3
#### a
Same model as above, except use log weight for covariate. Use entire data

```{r}
d <- Howell1
d$l_weight <- log(d$weight)
xbar <- d$l_weight |> mean()
m_h3 <- quap(alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b*(l_weight - xbar),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0,1),
  sigma ~ dunif(0,50)
), data=d)

precis(m_h3)
```

$\alpha$ can be interpreted as the value for when log weight is the average. A little harder, but doable. The coefficient for $\beta$ is no longer for a linear increase in weight, it is a linear increase log weight. So for each order of magnitude-ish (natural not base 10) increase we get a 47.07 increase in height.

#### b
Same plot as H.2 b. 
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

weight_seq <- seq(1.4, 4.2, length.out=1e3)
mu <- link(m_h3, data=data.frame(l_weight=weight_seq))


sim_height <- sim(m_h3, data=data.frame(l_weight=weight_seq))

plot(height ~ weight, data=d, col=col.alpha(rangi2,0.5))
lines(weight_seq |> exp(), colMeans(mu))
shade(apply(mu, 2, PI, 0.97), weight_seq |> exp())
shade(apply(sim_height, 2, PI, 0.97), weight_seq |> exp())
```


