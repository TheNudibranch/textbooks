---
title: "Rethinking - Chapter 11"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true
library(rethinking)
library(dagitty)
library(tidyverse)
```

## 11.0

- We'll spend our time on Binomial and Poisson regression in this chapter

## 11.1 - Binomial Regression

- Binomial distribution has maximum entropy when the result must be one of two events and the expected value is constant across trials
- Two common flavors of GLM that use binomial probability:
  - Logistic Regression: single trial 0/1
  - Aggregated Binomial Regression
- Both use logistic link and can be convereted between one another

Let's look at some chimp data. There are four plates on a rectangular table. There is a right and a left lever. If one of the levers is pulled, the two plates on that side of the table are sent to their respective ends. One side of the table might have food only on the plate that will be delivered to the chimp pulling the lever. The other side might have both plates filled, delivering food to both the chimp pulling the lever as well as the one seated across the table. This would be the *prosocial* option. Let's see what we can find as the interaction between condition (presence or absence of another animal) and option (which side would be the prosocial option).

```{r}
data(chimpanzees)
d <- chimpanzees
```

We are interested in estimating the interaction between `prosoc_left`, meaning that the prosocial option is the left lever, and `conidition`, a 1 if there is another chimp seated across the table.

The conventional way to do this would be to construct a linear interaction between the two variables, but that makes it hard to determine different priors. Instead, let's build an index variable from 1 to 4. We can visualize it with the `xtabs` function:

```{r}
d$treatment <- 1 + d$prosoc_left + 2*d$condition
xtabs(~ treatment + prosoc_left + condition, d)
```
The model is of the form:

\begin{align}
L &\sim \text{Binomial}(1,p_i)\\
\text{logit}(p_i) &= \alpha_{\text{ACTOR}[i]} + \beta_{\text{TREATMENT}[i]}\\
\alpha_j &\sim \text{to be determined}\\
\beta_k &\sim \text{to be determined}\\
\end{align}

We could of course defined $L$ to be a simple Bernoulli since it only has one trial.

Before we actually dive into the full model, let's try something simple to warm up our prior predictive juices:

\begin{align}
L &\sim \text{Binomial}(1,p_i)\\
\text{logit}(p_i) &= \alpha\\
\alpha &\sim \text{Normal}(0,\omega)\\
\end{align}

Let's choose $\omega=10$ for our value:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

m11.1 <- quap(alist(
  pulled_left ~ dbinom(1, p),
  logit(p) <- a,
  a ~ dnorm(0,10)
), data=d)

prior <- extract.prior(m11.1, n=1e4)
p <- inv_logit(prior$a)
dens(p, adj=0.1)
```

The model here is telling us that the chimp will almost never, or always pull the lever. There is no grey area. This implies that a flat prior in the logit space is not a flat prior in the outcome space. Let's stick with $\omega=1.5$ which will give us a much better behaved shape. Now, we could probably just use 1.5 again for $\beta$, but let's see what happens if we don't:

```{r}
m11.2 <- quap(alist(
  pulled_left ~ dbinom(1, p),
  logit(p) <- a + b[treatment],
  a ~ dnorm(0,1.5),
  b[treatment] ~ dnorm(0,10)
), data=d)

prior <- extract.prior(m11.2)
p <- lapply(1:4, \(x) inv_logit(prior$a + prior$b[,x])) |> as.data.frame()
```

Let's see what the priors imply about the prior *differences* between the treatments, since that is what we are really after.

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

dens(abs(p[,1] - p[,2]), adj=0.1)
```

Here the model thinks that the treatments are either completely alike, or completely different. Let's try a 0.5 scale:

```{r}
m11.3 <- quap(alist(
  pulled_left ~ dbinom(1, p),
  logit(p) <- a + b[treatment],
  a ~ dnorm(0,1.5),
  b[treatment] ~ dnorm(0,0.5)
), data=d)

prior <- extract.prior(m11.3)
p <- lapply(1:4, \(x) inv_logit(prior$a + prior$b[,x])) |> as.data.frame()
(p[,1] - p[,2]) |> abs() |> mean()
```

Now the average prior difference is about 10%. Extremely large differences are very unlikely now - this will help reduce overfitting.

Let's go ahead and fit the model using HMC:

```{r}
#| output: false
dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = d$treatment
)

m11.4 <- ulam(alist(
  pulled_left ~ dbinom(1, p),
  logit(p) <- a[actor] + b[treatment],
  a[actor] ~ dnorm(0,1.5),
  b[treatment] ~ dnorm(0, 0.5)
), data=dat_list, chains=4, log_lik=T)
```
```{r}
precis(m11.4, depth=2)
```

Let's look at the chimps behavior on the outcome scale:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
post <- extract.samples(m11.4)
p_left <- inv_logit(post$a)
plot(precis(as.data.frame(p_left)), xlim=c(0,1))
```

Now, let's see how the treatments look:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
labs <- c('R/N', 'L/N', 'R/P', 'L/P')
plot(precis(m11.4, depth=2, pars='b'), labels=labs)
```

Here "R/N" means prosocial on right and no partner. What we want to see is if the chimp chose the prosocial option in the presence of a partner or not. For this we want to compare the difference between the first and third (top down), and second and fourth:

```{r}
diffs <- list(
  db13 = post$b[,1] - post$b[,3],
  db24 = post$b[,2] - post$b[,4]
)
plot(precis(diffs))
```
These are **contrasts** between no-partner/partner treatments on the log-odds scale. Clearly there isn't any compelling evidence of prosocial choice in this experiment.

For our next inspection, let's see how our posterior predictions faired against the actual data on the actor/treatment level:

```{r}
pl <- by(d$pulled_left, list(d$actor, d$treatment), mean)
pl[,1] # average pulls by treatment for chimp 1
```

I won't write out the whole code for visualizing, but you'll see that we are pretty consistent with what the actual data shows. The big factor is handedness of the chimps.

Let's try a simpler model where we don't include an interaction - just the two separate variables that made up the interaction:

```{r}
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2
```

```{r}
#| output: false
dat_list2 <- list(
  pulled_left=d$pulled_left,
  actor = d$actor,
  side = d$side,
  cond = d$cond
)
m11.5 <- ulam(alist(
  pulled_left ~ dbinom(1, p),
  logit(p) <- a[actor] + bs[side] + bc[cond],
  a[actor] ~ dnorm(0,1.5),
  bs[side] ~ dnorm(0, 0.5),
  bc[cond] ~ dnorm(0, 0.5)
), data=dat_list2, chains=4, log_lik=T, cores=4)
```

```{r}
compare(m11.5, m11.4, func=PSIS)
```
Just as we thought - the simpler model is actually better. This is because the model actually has more data and can better estimate the parameters separately instead of trying to force them together.

So far we have been talking about **absolute effects**: the difference a counter-factual change in a variable makes on the measurement scale. i.e. if we change the condition to be prosocial, we see the probability increase by 10 points. Now we are going to talk about the **relative effects**: the multiplier a change in effect has on the outcome. i.e. if we change the condition to be prosocial we double the outcome of an event. For example, the proportional odds of switching from treatment 2 to treatment 4 (adding a partner):

```{r}
post <- extract.samples(m11.4)
mean(exp(post$b[,4] - post$b[,2]))
```
This means that the switch multiples the odds of pulling the left lever by 0.92. A 8% reduction in odds (this is **proportional odds**). Clearly, this is not enough though. If the other variables in the model make the outcome unlikely, ten a large proportional odds like 5x would not make the outcome likely. 

### Aggregate Binomial

There is nothing stopping us from doing a aggregated count model instead sice there is no need to preserve order. This can actually make things much faster since there is less data:

```{r}
d_aggregated <- aggregate(
  d$pulled_left,
  list(treatment=d$treatment, 
       actor=d$actor,
       side=d$side,
       cond=d$cond),
  sum
)
colnames(d_aggregated)[5] <- 'left_pulls'
d_aggregated |> head()
```

```{r}
library(tidyverse)
d %>% group_by(treatment, actor, side, cond) %>% summarise(left_pulls=sum(pulled_left), cnt=n())
```
Something that is very important to note is that there were exactly 18 trials for each chimp and set of conditions. If they were different, we would have to change $N$ for each point (not hard to do though).

Let's fit the full model:

```{r}
#| output: false
m11.6 <- ulam(alist(
  left_pulls ~ dbinom(18, p),
  logit(p) <- a[actor] + b[treatment],
  a[actor] ~ dnorm(0, 1.5),
  b[treatment] ~ dnorm(0,1.5)
), data=as.list(d_aggregated), chains=4, cores=4, log_lik=T)
```

And compare to the identical model `m11.4`:
```{r}
compare(m11.4, m11.6, func=PSIS)
```

The PSIS scores are very different. The main reason is that the probabilities are a lot higher for the aggregate model. This makes sense, the binomial probability takes into account all the different ways that x out of 18 trials can occur. That is taken into account by the binomial coefficient. 

You'll also notice that we got $\hat{k}$ warnings. This is because we really aren't doing LOO-CV anymore. We can be leaving up to 18 points out since the weights are calculated at the $(\text{treatment}, \text{actor})$ level. So leaving one out is really like leaving 18 points out. Much more likely for things to be influential in the total model. 

Let's now switch to a instance where we don't have the same number of trials per data point:

```{r}
data(UCBadmit)
d <- UCBadmit
```

We want to model the probability of acceptance given an applicants gender:

\begin{align}
A &\sim \text{Binomial}(N_i,p_i)\\
\text{logit}(p_i) &= \alpha_{\text{GID[i]}}\\
\alpha_j &\sim \text{Normal}(0,1.5)\\
\end{align}

```{r}
#| output: false
dat_list <- list(
  admit = d$admit,
  applications = d$applications,
  gid = ifelse(d$applicant.gender == 'male', 1, 2)
)

m11.7 <- ulam(alist(
  admit ~ dbinom(applications, p),
  logit(p) <- a[gid],
  a[gid] ~ dnorm(0,1.5)
), data=dat_list, chains=4, cores=4)
```

```{r}
precis(m11.7, depth=2)
```

Let's now calculate the contrast between male and female applicants. We will calculate it on the relative and absolute scale:

```{r}
post <- extract.samples(m11.7)
diff_a <- post$a[,1] - post$a[,2] 
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis(list(diff_a=diff_a, diff_p=diff_p))
```
Recall that `diff_a` is giving us the difference on the log-odds scale, while `diff_p` is on the absolute scale. So it looks like that being male gives students a 14% higher on average acceptance rate.

Let's plot some posterior predictive checks:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
postcheck(m11.7)
for (i in 1:6){
  x <- 1 + 2*(i-1)
  y1 <- d$admit[x]/d$applications[x]
  y2 <- d$admit[x+1]/d$applications[x+1]
  lines(c(x, x+1), c(y1,y2), col=rangi2, lwd=2)
  text(x + 0.5, (y1 + y2)/2 + 0.05, d$dept[x], cex=0.8, col=rangi2)
}
```

The vertical *lines* are the 89% CI for the expected value. The crosses are the 89% simulated samples, and the open points are the posterior mean.

You'll notice that it isn't actually the case that male acceptance is higher than female. That is only the case for departments C and E. This is of course a case of Simpson's Paradox. Women are more likely to apply to departments with lower admission rates like department F:

```{r}
d %>% group_by(dept) %>% summarise(
  overall_acceptance = sum(admit) / sum(applications), 
  perc_female = sum(ifelse(applicant.gender=='female', applications,0))/sum(applications))
```

Let's now fit a model that models the variation between male and female within departments.

```{r}
#| output: false
dat_list$dept_id <- rep(1:6, each=2)

m11.8 <- ulam(alist(
  admit ~ dbinom(applications, p),
  logit(p) <- a[gid] + delta[dept_id],
  a[gid] ~ dnorm(0,1.5),
  delta[dept_id] ~ dnorm(0,1.5)
), data=dat_list, chains=4, cores=4)
```
```{r}
precis(m11.8, depth=2)
```

```{r}
post <- extract.samples(m11.8)
diff_a <- post$a[,1] - post$a[,2] 
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis(list(diff_a=diff_a, diff_p=diff_p))
```

We now see that male applicants might have it *worse*. But this of course is not a significant effect.

Let's make a DAG showing our causal graph:

```{r}
library(dagitty)
dag_m8.1 <- dagitty("dag{
  G -> D -> A
  G -> A
}")
coordinates(dag_m8.1) <- list(x=c('A'=1, 'G'=0, 'D'=0.5),
                              y=c('A'=0, 'G'=0, 'D'=-1))
drawdag(dag_m8.1)
```
Notice there is an indirect causal path $G \rightarrow D \rightarrow A$, from gender to acceptance. So we need to condition on $D$ and close the path, which is what model `m11.8` did.

Recall though, there might be a confounder $U$:

```{r}
dag_m8.1 <- dagitty("dag{
  U [unobserved]
  G -> D -> A
  G -> A
  D <- U -> A
}")
coordinates(dag_m8.1) <- list(x=c('A'=1, 'G'=0, 'D'=0.5, 'U'=1),
                              y=c('A'=0, 'G'=0, 'D'=-1, 'U' = -1))
drawdag(dag_m8.1)
```
If we condition on $D$, we actually would have $D$ as a collider and open up the non-causal path $G \rightarrow D \leftarrow U \rightarrow A$.

Going back to model `m11.8`, it is technically overparameterized. This of course is not a big issue for Bayesian models, so long as we have an adequate sampler that can handle the high correlations. It might be more efficient to instead to use a single parameter that represents the difference between male and female. Richard argues though that this will make one group more uncertain than the other. This of course works itself out on the outcome scale though.

## 11.2 - Poisson Regression

Let's look at the skeleton of a Poisson model:

\begin{align}
y_i &\sim \text{Poisson}(\lambda_i)\\
\log(\lambda_i) &= \alpha + \beta (x_i - \bar{x})\\
\end{align}

The data we will start with is that of Oceanic societies and their number of distinct tools:

```{r}
data("Kline")
d <- Kline
d
```

Let's look at some the assumptions we would like to bake into our model that predicts `total_tools`:

- Number of tools should increase with log population
- Islands with more contact will have more tools
- We expect the impact of population to be moderated by contact (interactions)

```{r}
d$P <- d$population |> log() |> scale()
d$contact_id <- ifelse(d$contact == 'high', 2, 1)
```

\begin{align}
T_i &\sim \text{Poisson}(\lambda_i)\\
\log(\lambda_i) &= \alpha_{\text{CID[i]}} + \beta_{\text{CID[i]}} \log P_i\\
\alpha_j &\sim \text{To be determined} \\ 
\beta_j &\sim \text{To be determined} \\ 
\end{align}

Let's first try to sniff out the correct priors. We know that $\lambda$ has to be positive, so any prior we put on $\alpha$ will be exponentiated. So if we choose a normal prior with $\sigma=10$, then $\lambda$ we have a lognormal outcome.

```{r}
curve(dlnorm(x, 0, 10), 0, 100, n=200)
```
Now, this tail is *very* long. So long in fact that the mean of the distribution above is:
```{r}
mean(rlnorm(1e4, 0,10))
```
Something to keep in mind is that the log link puts all the negative between 0 and 1 on the outcome scale. That means that if half of our prior mass is below 0, then half of the outcome will be between 0 and 1.

Let's instead try something a bit more sensible:

```{r}
curve(dlnorm(x, 3, 0.5), 0, 100, n=200)
mean(rlnorm(1e5, 3, 0.5)) |> print()
```

We now need to come up with something sensible for $\beta$. Let's again try $N(0,10)$ and see what happens to the outcome:

```{r}
N <- 100
a <- rnorm(N, 3, 0.5)
b <- rnorm(N, 0, 10)
plot(NULL, xlim=c(-2,2), ylim=c(0,100))
for (i in 1:N) curve(exp(a[i] + b[i]*x), add=T, col=grau(), xlab='x')
```
The pivoting around 0 is just the mean outcome for the average log population (approx. 22). What's interesting is that model thinks that just above or below the mean the outcome variable either goes to zero or blows up! Nothing in-between.

Something better might be:

```{r}
N <- 100
a <- rnorm(N, 3, 0.5)
b <- rnorm(N, 0, 0.2)
plot(NULL, xlim=c(-2,2), ylim=c(0,100))
for (i in 1:N) curve(exp(a[i] + b[i]*x), add=T, col=grau(), xlab='x')
```
Hmm, a little too tight. Almost on relationship now between log population and tools. 

Let's look at this a different way - instead of the z-score, let's keep population unstandardized:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
par(mfrow=c(1,2))

x_seq <- seq(log(100), log(2e5), length.out=105)
lambda <- vapply(x_seq, \(x) exp(a + b*x), numeric(N))
plot(NULL, xlim=range(x_seq), ylim=range(lambda), xlab='log pop', ylab='total tools')
for (i in 1:N) lines(x_seq, lambda[i,], col=grau(), lwd=1.5)

plot(NULL, xlim=range(exp(x_seq)), ylim=range(lambda), xlab='Population', ylab='total tools')
for (i in 1:N) lines(exp(x_seq), lambda[i,], col=grau(), lwd=1.5)
```

Notice that the left plot we be linear if we logged it. This also shows why it is usually better to log the covariate. We want to imply that there are diminishing returns for adding population at higher values. It means more to add a couple people when the population is very low than compared to when it is high.

We can now go ahead and build our models. We will fit an intercept only model as well as the full model:

```{r}
#| output: false
dat <- list(
  T = d$total_tools,
  P = d$P,
  cid = d$contact_id
)

# intercept only
m11.9 <- ulam(alist(
  T ~ dpois(lambda),
  log(lambda) <- a,
  a ~ dnorm(3, 0.5)
), data=dat, chains=4, cores=4, log_lik = TRUE)

# full model
m11.10 <- ulam(alist(
  T ~ dpois(lambda),
  log(lambda) <- a[cid] + b[cid]*P,
  a[cid] ~ dnorm(3, 0.5),
  b[cid] ~ dnorm(0, 0.2)

), data=dat, chains=4, cores=4, log_lik=TRUE)
```

```{r}
compare(m11.9, m11.10)
```

Notice that model `m11.10` actually has fewer number of effective parameters. This is surprising since we actually have 3 net new parameters in model `m11.10`. But, recall that we can really only think about effective parameters and parameter count relationship in simple linear models. We know that parameter values near boundaries produce less overfitting and therefore contribute less to effective number of parameters. This is true about bounded data too. See that a large amount of the bounds are bounded by population not able to be negative, so introducing this covariate actually reduces the number of effective parameters since it limits the parameter space inadvertently.

Let's go ahead and plot the posterior predictive space:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
par(mfrow=c(1,2))

K <- PSIS(m11.10, pointwise = T)$k
plot(dat$P, dat$T, xlab='log pop (std.)', ylab='tools', col=rangi2, 
     pch=ifelse(dat$cid == 1, 1, 16), lwd=2, cex=1+normalize(K))

ns <- 100
p_seq <- seq(-1.4, max(dat$P), length.out=ns)

for (i in 1:2){
  lambda <- link(m11.10, data=data.frame(P=p_seq, cid=i))
  lmu <- colMeans(lambda)
  lci <- apply(lambda, 2, PI)
  lines(p_seq, lmu, lty=ifelse(i==1,2,1), lwd=1.5)
  shade(lci, p_seq, xpd=T)
}

plot(d$population, d$total_tools, xlab='pop', ylab='tools',
     col=rangi2, pch=ifelse(dat$cid == 1, 1, 16), lwd=2,
     cex=1 + normalize(K))

ns <- 100
p_seq <- seq(-1.4, max(dat$P), length.out=ns)
# Need to convert back to non standard (add mean and multilpy by sd of log pop)
pop_seq <- exp(p_seq*1.53 + 9)

for (i in 1:2){
  lambda <- link(m11.10, data=data.frame(P=p_seq, cid=i))
  lmu <- colMeans(lambda)
  lci <- apply(lambda, 2, PI)
  lines(pop_seq, lmu, lty=ifelse(i==1,2,1), lwd=1.5)
  shade(lci, pop_seq, xpd=T)
}
```

Hawaii is the point in the top right of the right graph. Clearly, because of its very large population, it is highly influential and is therefore contributing to the high $\hat{k}$.

Now, notice that the trend for high contact societies (solid) is higher than that of low contact societies (dashed). But, notice that the model is allowing that past a certain population size, the relationship flips and low contact becomes higher expected than high contact. Of course, a counterfactual of Hawaii with low contact shouldn't be higher than current Hawaii. 

Also, notice that sine we have given the model the intercept as a free parameter, it can be the case where will will have a non-zero number of expected tools when the population is zero. This can never be possible!

We could combat this by coming up with an ecology informed formula. The implementation is at the top of page 356.

### 11.2.2 Negative Binomial

It is sometimes important to allow for extra variation beyond just assuming the variance is equal to $\lambda$. This would lead us to the Negative Binomial, or sometimes called the Gamma-Poisson. It is really just a Poisson in disguise since it is a mixture of many Poisson distributions (gamma is the mixing distribution and $\lambda$ is Gamma distributed). It is analagous to the student-t distribution which is a mixture of normals where $\sigma^2$ follows an inverse gamma.

### 11.2.3 Exposure and Offset

The parameter $\lambda$ is the expected number of events, but it can also be thought of as a rate. Implicitly, $\lambda = \mu/\tau$, the expected number of events $\mu$ per unit time or distance $\tau$. We can then redefine our model as:


\begin{align}
y_i &\sim \text{Poisson}(\lambda_i)\\
\log \lambda_i &= \log \frac{\mu_i}{\tau_i} = \alpha + \beta x_i\\
\end{align}

We can then use some log math to show $\log \lambda_i = \log \mu_i = \log \tau_i + \alpha + \beta x_i$.

Notice when $\tau_i = 1$ then we are left with the same equation we've been using the whole time. But, if $\tau_i$ does change we are effectively just adding another covariate ($\log \tau_i$) to our linear equation. BUT, there is no coefficient for this covariate - it is just a raw addition value.

Our model then effectively becomes:

\begin{align}
y_i &\sim \text{Poisson}(\mu_i)\\
\log \mu_i &= \log \tau_i + \alpha + \beta x_i\\
\end{align}


This $\log \tau_i$ column is also called the **offset**.

Let's simulate some data:

```{r}
# monastery 1
num_days <- 30
y <- rpois(num_days, 1.5)

# monastery 2 - measured on a weekly basis but lower daily rate
num_weeks <- 4
y_new <- rpois(num_weeks, 0.5*7)

y_all <- c(y, y_new)
exposure <- c(rep(1,30), rep(7,4))
monastery <- c(rep(0,30), rep(1,4))
d <- data.frame(y=y_all, days=exposure, monastery=monastery)
d$log_days <- log(d$days)

m11.12 <- quap(alist(
  y ~ dpois(lambda),
  log(lambda) <- log_days + a + b*monastery,
  a ~ dnorm(0, 1),
  b ~ dnorm(0, 1)
), data=d)
```

When we extract the predictions, everything is already on the daily scale so we don't need to make any adjustments.

```{r}
post <- extract.samples(m11.12)
lambda_old <- exp(post$a)
lambda_new <- exp(post$a + post$b)

precis(data.frame(lambda_old, lambda_new))
```

The parameters are about right with what we simulated.

## 11.3 - Multinomial and categorical models

When more than two types of events are possible, and the probability of each type of event is constant and unordered, then the maximum entropy distribution is a multinomial distribution. The pdf is defined as:

$$P(y_1, \dots. y_K|n, p_1, \dots, p_K) = \frac{n!}{\prod_i y_i!} \prod^K_{i=1}p_i^{y_i}$$

Where $K$ is the unique number of events, and $y_i$ is the number of events of type $i$ out of $n$ trials. A model with this likelihood is sometimes referred to as **categorical regression**. There are two approachs we are going to use to specify the model. The *explicit* approach uses the multinomial logit, aka the softmax as  the link:

$$P(k|s_1, \dots, s_K) = \frac{\exp(s_k)}{\sum_{i=1}^K\exp(s_i)}$$
The scores here are of course continuous. For multinomial logistic regression we need $K-1$ models. One of the outcome variables will be our pivot variable. This of course keeps us from having an overdetermined system of equations.

### 11.3.1 Predictors matched to outcomes

Let's simulate career choices for young adults:

```{r}
N <- 500
income <- c(1,2,5)
score <- 0.5*income
p <- softmax(score)

career <- rep(NA, N)
set.seed(34302)
for (i in 1:N) career[i] <- sample(1:3, size=1, prob=p)
```


Let's now fit the model with stan code:

```{r}
#| output: false
code_m11.13 <- "
data{
   int N;
   int K;
   array[N] int career;
   vector[K] career_income;
}
parameters{
  vector[K-1] a; // intercepts
  real<lower=0> b; // association of income with choice
}
model{
  vector[K] p;
  vector[K] s;
  a ~ normal(0, 1);
  b ~ normal(0, 0.5);
  s[1] = a[1] + b*career_income[1];
  s[2] = a[2] + b*career_income[2];
  s[3] = 0; // pivot 
  p = softmax(s);
  career ~ categorical(p);
}

"

dat_list <- list(N=N, K=3, career=career, career_income=income)
m11.13 <- stan(model_code=code_m11.13, data=dat_list, chains=4, cores=4)
```

```{r}
precis(m11.13, depth=2)
```

It is clear that `b` is positive, but we don't know how it actually effects the outcome. This is why we always need to convert to the outcome scale.

Let's make a counterfactual - what would happen if we double the income of a career, specifically career 2:

```{r}
post <- extract.samples(m11.13)

s1 <- with(post, a[,1] + b*income[1])
s2_orig <- with(post, a[,2] + b*income[2])
s2_new <- with(post, a[,2] + b*income[2]*2)

p_orig <- vapply(1:length(post$b), \(x) softmax(s1[x], s2_orig[x], 0), numeric(3)) |> t()
p_new <- vapply(1:length(post$b), \(x) softmax(s1[x], s2_new[x], 0), numeric(3)) |> t()
precis(p_new[,2] - p_orig[,2])
```
So about a 5 point increase for doubling the income. Notice, that value is conditional on comparing to the other careers. 

### 11.3.1 Predictors matched to observations

Suppose now that we want to incorporate family income into the model. The difference here is that the predictor variable  (family income), must have the same value for each linear model.

```{r}
N <- 500
family_income <- runif(N)
b <- c(-2,0,2)

career <- rep(NA, N)
for (i in 1:N){
  score <- 0.5*(1:3) + b*family_income[i]
  p <- softmax(score)
  career[i] <- sample(1:3, size=1, prob=p)
}
```

Now the stan code:

```{r}
#| output: false
code_m11.14 <- "
data{
   int N;
   int K;
   array[N] int career;
   vector[N] family_income;
}
parameters{
  vector[K-1] a; // intercepts
  vector[K-1] b; // coeff for family income
}
model{
  vector[K] p;
  vector[K] s;
  a ~ normal(0, 1.5);
  b ~ normal(0, 1);
  for (i in 1:N){
    for (j in 1:(K-1)) s[j] = a[j] + b[j] * family_income[i];
    s[K] = 0; // pivot
    p = softmax(s);
    career[i] ~ categorical(p);
  }
}

"
dat_list <- list(N=N, K=3, career=career, family_income=family_income)
m11.14 <- stan(model_code=code_m11.14, data=dat_list, chains=4, cores=4)
```
```{r}
precis(m11.14, depth=2)
```

Again, we can't look at those coefficients in a vacuum. We need to convert to the outcome scale.

### 11.3.2 Multinomial in disguise as Poisson

Let's start with a simple binomial problem, since a binomial is a special case of the multinomial:

```{r}
#| output: false
data("UCBadmit")
d <- UCBadmit

m_binom <- quap(alist(
  admit ~ dbinom(applications, p),
  logit(p) <- a,
  a ~ dnorm(0, 1.5)
), data=d)

# Poisson model of overall admission rate and rejection rate
dat <- list(admit=d$admit, rej=d$reject)
m_pois <- ulam(alist(
  admit ~ dpois(lambda1),
  rej ~ dpois(lambda2),
  log(lambda1) <- a1,
  log(lambda2) <- a2,
  c(a1, a2) ~ dnorm(0, 1.5)
), data=dat, chains=4, cores=4)
```

Let's look at the posterior means for simplicity:

```{r}
inv_logit(coef(m_binom))
```

```{r}
k <- coef(m_pois)
a1 <- k['a1']; a2 <- k['a2']
exp(a1)/(exp(a2) + exp(a1)) # softmax
```

## Excersises

### M5

If we use the logit link for the mean of a Poisson, that would imply that we want the mean of the Poisson, $\lambda$, to be between 0 and 1. We might use this for any process that we have prior knowledge that on average produces less than 1 unit per time period. Could be some high half life decaying element where we only see emissions at very low rates. 


### H2

#### a
For this question we will look at Bald Eagles and their attempts to steal other eagle's food sources. We want to model the probability of a successful stealing attempt from a "victim" eagle.

```{r}
library(MASS)
data("eagles")
d <- eagles
d$P_ind <- (as.character(d$P) == 'L') |> as.integer()
d$A_ind <- (as.character(d$A) == 'A') |> as.integer()
d$V_ind <- (as.character(d$V) == 'L') |> as.integer()
str(d)
```
Here $y$ is the number of successful attempts out of $n$ trials. $P$ is a dummy variable indicating if the thief eagle has a large body size. $V$ is the same, but for the victim eagle. $A$ is an indicator for if the thief eagle is an adult.

Let's fit a simply Binomial GLM using the `ulam` and `quap` inference methods.

```{r}
#| output: false
mod_list <- alist(
  y ~ dbinom(n, p),
  logit(p) <- alpha + b_P*P_ind + b_A*A_ind + b_V*V_ind,
  alpha ~ dnorm(0,1.5),
  c(b_P, b_A, b_V) ~ dnorm(0,0.5)
)

mh5_q <- quap(mod_list, data=d)
mh5_u <- ulam(mod_list, data=d |> as.list(), cores=4, chains=4)
```

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

mh5_q_samps <- extract.samples(mh5_q) |> as.data.frame()
mh5_u_samps <- extract.samples(mh5_u) |> as.data.frame()

par(mfrow=c(2,2))
for (i in colnames(mh5_q_samps)){
  dens(mh5_q_samps[[i]], main=paste0('Coef ', i), 
       col='darkblue', lwd=2)
  dens(mh5_u_samps[[i]], add=T, col='darkred', lwd=2)
}
```

It looks like they are actually the same. We are okay to use `quap` for this problem.

#### b

Let's look at the output from `quap`:

```{r}
precis(mh5_q)
```

It looks like $P$ and $V$ have about the same effect as each other, but negate each other. $A$ is a positive effect - which makes sense since we would expect the thief being an adult to increase the success rate. 

Let's look at the posterior predictive, we'll first look at the rate:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

postcheck(mh5_q)
```

Instead of the rates, let's look at the simulated counts:

```{r}
a <- sim(mh5_q)
a_pi <- apply(a, 2, PI)

plot(colMeans(a), ylim=range(a_pi, d$y), ylab='Expected Counts')
points(d$y, col=rangi2, pch=16)
rep(1:8) |> arrows(x0=_, y0=a_pi[2,], y1=a_pi[1,], code=0, lwd=2)
```
The proportion plot and the counts plot tell a similar story. In fact, if the pluses overlap with the actual value in the first plot, they are almost guaranteed to in the second. This is becuase they are the same quantities just with different normalizations. What the second plot helps to show is that we get worse predictions as the sample size increases, which is something you wouldn't immediately see in the first plot.

#### c

Let's add an interaction to the model between the thief's size and age:

```{r}
mod_list <- alist(
  y ~ dbinom(n, p),
  logit(p) <- alpha + b_P*P_ind + b_A*A_ind + b_V*V_ind + b_PA*P_ind*A_ind,
  alpha ~ dnorm(0,1.5),
  c(b_P, b_A, b_V, b_PA) ~ dnorm(0,0.5)
)

mh5_q2 <- quap(mod_list, data=d)
precis(mh5_q2)
```
Let's see how much it helps:

```{r}
compare(mh5_q, mh5_q2, func=PSIS)
```
```{r}
compare(mh5_q, mh5_q2, func=WAIC)
```

Looks like the non-interaction model is *slightly* better, but the difference between the two models is not significant.

### H3
### H6

```{r}

```

