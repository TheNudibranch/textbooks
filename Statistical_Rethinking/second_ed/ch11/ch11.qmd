---
title: "Rethinking - Chapter 11"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true
library(rethinking)
library(dagitty)
library(tidyverse)
```

## 11.0

- We'll spend our time on Binomial and Poisson regression in this chapter

## 11.1 - Binomial Regression

- Binomial distribution has maximum entropy when the result must be one of two events and the expected value is constant across trials
- Two common flavors of GLM that use binomial probability:
  - Logistic Regression: single trial 0/1
  - Aggregated Binomial Regression
- Both use logistic link and can be convereted between one another

Let's look at some chimp data. There are four plates on a rectangular table. There is a right and a left lever. If one of the levers is pulled, the two plates on that side of the table are sent to their respective ends. One side of the table might have food only on the plate that will be delivered to the chimp pulling the lever. The other side might have both plates filled, delivering food to both the chimp pulling the lever as well as the one seated across the table. This would be the *prosocial* option. Let's see what we can find as the interaction between condition (presence or absence of another animal) and option (which side would be the prosocial option).

```{r}
data(chimpanzees)
d <- chimpanzees
```

We are interested in estimating the interaction between `prosoc_left`, meaning that the prosocial option is the left lever, and `conidition`, a 1 if there is another chimp seated across the table.

The conventional way to do this would be to construct a linear interaction between the two variables, but that makes it hard to determine different priors. Instead, let's build an index variable from 1 to 4. We can visualize it with the `xtabs` function:

```{r}
d$treatment <- 1 + d$prosoc_left + 2*d$condition
xtabs(~ treatment + prosoc_left + condition, d)
```
The model is of the form:

\begin{align}
L &\sim \text{Binomial}(1,p_i)\\
\text{logit}(p_i) &= \alpha_{\text{ACTOR}[i]} + \beta_{\text{TREATMENT}[i]}\\
\alpha_j &\sim \text{to be determined}\\
\beta_k &\sim \text{to be determined}\\
\end{align}

We could of course defined $L$ to be a simple Bernoulli since it only has one trial.

Before we actually dive into the full model, let's try something simple to warm up our prior predictive juices:

\begin{align}
L &\sim \text{Binomial}(1,p_i)\\
\text{logit}(p_i) &= \alpha\\
\alpha &\sim \text{Normal}(0,\omega)\\
\end{align}

Let's choose $\omega=10$ for our value:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

m11.1 <- quap(alist(
  pulled_left ~ dbinom(1, p),
  logit(p) <- a,
  a ~ dnorm(0,10)
), data=d)

prior <- extract.prior(m11.1, n=1e4)
p <- inv_logit(prior$a)
dens(p, adj=0.1)
```

The model here is telling us that the chimp will almost never, or always pull the lever. There is no grey area. This implies that a flat prior in the logit space is not a flat prior in the outcome space. Let's stick with $\omega=1.5$ which will give us a much better behaved shape. Now, we could probably just use 1.5 again for $\beta$, but let's see what happens if we don't:

```{r}
m11.2 <- quap(alist(
  pulled_left ~ dbinom(1, p),
  logit(p) <- a + b[treatment],
  a ~ dnorm(0,1.5),
  b[treatment] ~ dnorm(0,10)
), data=d)

prior <- extract.prior(m11.2)
p <- lapply(1:4, \(x) inv_logit(prior$a + prior$b[,x])) |> as.data.frame()
```

Let's see what the priors imply about the prior *differences* between the treatments, since that is what we are really after.

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

dens(abs(p[,1] - p[,2]), adj=0.1)
```

Here the model thinks that the treatments are either completely alike, or completely different. Let's try a 0.5 scale:

```{r}
m11.3 <- quap(alist(
  pulled_left ~ dbinom(1, p),
  logit(p) <- a + b[treatment],
  a ~ dnorm(0,1.5),
  b[treatment] ~ dnorm(0,0.5)
), data=d)

prior <- extract.prior(m11.3)
p <- lapply(1:4, \(x) inv_logit(prior$a + prior$b[,x])) |> as.data.frame()
(p[,1] - p[,2]) |> abs() |> mean()
```

Now the average prior difference is about 10%. Extremely large differences are very unlikely now - this will help reduce overfitting.

Let's go ahead and fit the model using HMC:

```{r}
#| output: false
dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = d$treatment
)

m11.4 <- ulam(alist(
  pulled_left ~ dbinom(1, p),
  logit(p) <- a[actor] + b[treatment],
  a[actor] ~ dnorm(0,1.5),
  b[treatment] ~ dnorm(0, 0.5)
), data=dat_list, chains=4, log_lik=T)
```
```{r}
precis(m11.4, depth=2)
```

Let's look at the chimps behavior on the outcome scale:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
post <- extract.samples(m11.4)
p_left <- inv_logit(post$a)
plot(precis(as.data.frame(p_left)), xlim=c(0,1))
```

Now, let's see how the treatments look:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
labs <- c('R/N', 'L/N', 'R/P', 'L/P')
plot(precis(m11.4, depth=2, pars='b'), labels=labs)
```

Here "R/N" means prosocial on right and no partner. What we want to see is if the chimp chose the prosocial option in the presence of a partner or not. For this we want to compare the difference between the first and third (top down), and second and fourth:

```{r}
diffs <- list(
  db13 = post$b[,1] - post$b[,3],
  db24 = post$b[,2] - post$b[,4]
)
plot(precis(diffs))
```
These are **contrasts** between no-partner/partner treatments on the log-odds scale. Clearly there isn't any compelling evidence of prosocial choice in this experiment.

For our next inspection, let's see how our posterior predictions faired against the actual data on the actor/treatment level:

```{r}
pl <- by(d$pulled_left, list(d$actor, d$treatment), mean)
pl[,1] # average pulls by treatment for chimp 1
```

I won't write out the whole code for visualizing, but you'll see that we are pretty consistent with what the actual data shows. The big factor is handedness of the chimps.

Let's try a simpler model where we don't include an interaction - just the two separate variables that made up the interaction:

```{r}
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2
```

```{r}
#| output: false
dat_list2 <- list(
  pulled_left=d$pulled_left,
  actor = d$actor,
  side = d$side,
  cond = d$cond
)
m11.5 <- ulam(alist(
  pulled_left ~ dbinom(1, p),
  logit(p) <- a[actor] + bs[side] + bc[cond],
  a[actor] ~ dnorm(0,1.5),
  bs[side] ~ dnorm(0, 0.5),
  bc[cond] ~ dnorm(0, 0.5)
), data=dat_list2, chains=4, log_lik=T, cores=4)
```

```{r}
compare(m11.5, m11.4, func=PSIS)
```
Just as we thought - the simpler model is actually better. This is because the model actually has more data and can better estimate the parameters separately instead of trying to force them together.

So far we have been talking about **absolute effects**: the difference a counter-factual change in a variable makes on the measurement scale. i.e. if we change the condition to be prosocial, we see the probability increase by 10 points. Now we are going to talk about the **relative effects**: the multiplier a change in effect has on the outcome. i.e. if we change the condition to be prosocial we double the outcome of an event. For example, the proportional odds of switching from treatment 2 to treatment 4 (adding a partner):

```{r}
post <- extract.samples(m11.4)
mean(exp(post$b[,4] - post$b[,2]))
```
This means that the switch multiples the odds of pulling the left lever by 0.92. A 8% reduction in odds (this is **proportional odds**). Clearly, this is not enough though. If the other variables in the model make the outcome unlikely, ten a large proportional odds like 5x would not make the outcome likely. 

### Aggregate Binomial

There is nothing stopping us from doing a aggregated count model instead sice there is no need to preserve order. This can actually make things much faster since there is less data:

```{r}
d_aggregated <- aggregate(
  d$pulled_left,
  list(treatment=d$treatment, 
       actor=d$actor,
       side=d$side,
       cond=d$cond),
  sum
)
colnames(d_aggregated)[5] <- 'left_pulls'
d_aggregated |> head()
```

```{r}
library(tidyverse)
d %>% group_by(treatment, actor, side, cond) %>% summarise(left_pulls=sum(pulled_left), cnt=n())
```
Something that is very important to note is that there were exactly 18 trials for each chimp and set of conditions. If they were different, we would have to change $N$ for each point (not hard to do though).

Let's fit the full model:

```{r}
#| output: false
m11.6 <- ulam(alist(
  left_pulls ~ dbinom(18, p),
  logit(p) <- a[actor] + b[treatment],
  a[actor] ~ dnorm(0, 1.5),
  b[treatment] ~ dnorm(0,1.5)
), data=as.list(d_aggregated), chains=4, cores=4, log_lik=T)
```

And compare to the identical model `m11.4`:
```{r}
compare(m11.4, m11.6, func=PSIS)
```

The PSIS scores are very different. The main reason is that the probabilities are a lot higher for the aggregate model. This makes sense, the binomial probability takes into account all the different ways that x out of 18 trials can occur. That is taken into account by the binomial coefficient. 

You'll also notice that we got $\hat{k}$ warnings. This is because we really aren't doing LOO-CV anymore. We can be leaving up to 18 points out since the weights are calculated at the $(\text{treatment}, \text{actor})$ level. So leaving one out is really like leaving 18 points out. Much more likely for things to be influential in the total model. 

Let's now switch to a instance where we don't have the same number of trials per data point:

```{r}
data(UCBadmit)
d <- UCBadmit
```

We want to model the probability of acceptance given an applicants gender:

\begin{align}
A &\sim \text{Binomial}(N_i,p_i)\\
\text{logit}(p_i) &= \alpha_{\text{GID[i]}}\\
\alpha_j &\sim \text{Normal}(0,1.5)\\
\end{align}

```{r}
#| output: false
dat_list <- list(
  admit = d$admit,
  applications = d$applications,
  gid = ifelse(d$applicant.gender == 'male', 1, 2)
)

m11.7 <- ulam(alist(
  admit ~ dbinom(applications, p),
  logit(p) <- a[gid],
  a[gid] ~ dnorm(0,1.5)
), data=dat_list, chains=4, cores=4)
```

```{r}
precis(m11.7, depth=2)
```

Let's now calculate the contrast between male and female applicants. We will calculate it on the relative and absolute scale:

```{r}
post <- extract.samples(m11.7)
diff_a <- post$a[,1] - post$a[,2] 
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis(list(diff_a=diff_a, diff_p=diff_p))
```
Recall that `diff_a` is giving us the difference on the log-odds scale, while `diff_p` is on the absolute scale. So it looks like that being male gives students a 14% higher on average acceptance rate.

Let's plot some posterior predictive checks:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
postcheck(m11.7)
for (i in 1:6){
  x <- 1 + 2*(i-1)
  y1 <- d$admit[x]/d$applications[x]
  y2 <- d$admit[x+1]/d$applications[x+1]
  lines(c(x, x+1), c(y1,y2), col=rangi2, lwd=2)
  text(x + 0.5, (y1 + y2)/2 + 0.05, d$dept[x], cex=0.8, col=rangi2)
}
```

The vertical *lines* are the 89% CI for the expected value. The crosses are the 89% simulated samples, and the open points are the posterior mean.

You'll notice that it isn't actually the case that male acceptance is higher than female. That is only the case for departments C and E. This is of course a case of Simpson's Paradox. Women are more likely to apply to departments with lower admission rates like department F:

```{r}
d %>% group_by(dept) %>% summarise(
  overall_acceptance = sum(admit) / sum(applications), 
  perc_female = sum(ifelse(applicant.gender=='female', applications,0))/sum(applications))
```

Let's now fit a model that models the variation between male and female within departments.

```{r}
#| output: false
dat_list$dept_id <- rep(1:6, each=2)

m11.8 <- ulam(alist(
  admit ~ dbinom(applications, p),
  logit(p) <- a[gid] + delta[dept_id],
  a[gid] ~ dnorm(0,1.5),
  delta[dept_id] ~ dnorm(0,1.5)
), data=dat_list, chains=4, cores=4)
```
```{r}
precis(m11.8, depth=2)
```

```{r}
post <- extract.samples(m11.8)
diff_a <- post$a[,1] - post$a[,2] 
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis(list(diff_a=diff_a, diff_p=diff_p))
```

We now see that male applicants might have it *worse*. But this of course is not a significant effect.

Let's make a DAG showing our causal graph:

```{r}
library(dagitty)
dag_m8.1 <- dagitty("dag{
  G -> D -> A
  G -> A
}")
coordinates(dag_m8.1) <- list(x=c('A'=1, 'G'=0, 'D'=0.5),
                              y=c('A'=0, 'G'=0, 'D'=-1))
drawdag(dag_m8.1)
```
Notice there is an indirect causal path $G \rightarrow D \rightarrow A$, from gender to acceptance. So we need to condition on $D$ and close the path, which is what model `m11.8` did.

Recall though, there might be a confounder $U$:

```{r}
dag_m8.1 <- dagitty("dag{
  U [unobserved]
  G -> D -> A
  G -> A
  D <- U -> A
}")
coordinates(dag_m8.1) <- list(x=c('A'=1, 'G'=0, 'D'=0.5, 'U'=1),
                              y=c('A'=0, 'G'=0, 'D'=-1, 'U' = -1))
drawdag(dag_m8.1)
```
If we condition on $D$, we actually would have $D$ as a collider and open up the non-causal path $G \rightarrow D \leftarrow U \rightarrow A$.

Going back to model `m11.8`, it is technically overparameterized. This of course is not a big issue for Bayesian models, so long as we have an adequate sampler that can handle the high correlations. It might be more efficient to instead to use a single parameter that represents the difference between male and female. Richard argues though that this will make one group more uncertain than the other. This of course works itself out on the outcome scale though.

## 11.2 - Poisson Regression

Let's look at the skeleton of a Poisson model:

\begin{align}
y_i &\sim \text{Poisson}(\lambda_i)\\
\log(\lambda_i) &= \alpha + \beta (x_i - \bar{x})\\
\end{align}

The data we will start with is that of Oceanic societies and their number of distinct tools:

```{r}
data("Kline")
d <- Kline
d
```

Let's look at some the assumptions we would like to bake into our model that predicts `total_tools`:

- Number of tools should increase with log population
- Islands with more contact will have more tools
- We expect the impact of population to be moderated by contact (interactions)

```{r}
d$P <- d$population |> log() |> scale()
d$contact_id <- ifelse(d$contact == 'high', 2, 1)
```

\begin{align}
T_i &\sim \text{Poisson}(\lambda_i)\\
\log(\lambda_i) &= \alpha_{\text{CID[i]}} + \beta_{\text{CID[i]}} \log P_i\\
\alpha_j &\sim \text{To be determined} \\ 
\beta_j &\sim \text{To be determined} \\ 
\end{align}

Let's first try to sniff out the correct priors. We know that $\lambda$ has to be positive, so any prior we put on $\alpha$ will be exponentiated. So if we choose a normal prior with $\sigma=10$, then $\lambda$ we have a lognormal outcome.

```{r}
curve(dlnorm(x, 0, 10), 0, 100, n=200)
```
Now, this tail is *very* long. So long in fact that the mean of the distribution above is:
```{r}
mean(rlnorm(1e4, 0,10))
```
Something to keep in mind is that the log link puts all the negative between 0 and 1 on the outcome scale. That means that if half of our prior mass is below 0, then half of the outcome will be between 0 and 1.

Let's instead try something a bit more sensible:

```{r}
curve(dlnorm(x, 3, 0.5), 0, 100, n=200)
mean(rlnorm(1e5, 3, 0.5)) |> print()
```

We now need to come up with something sensible for $\beta$. Let's again try $N(0,10)$ and see what happens to the outcome:

```{r}
N <- 100
a <- rnorm(N, 3, 0.5)
b <- rnorm(N, 0, 10)
plot(NULL, xlim=c(-2,2), ylim=c(0,100))
for (i in 1:N) curve(exp(a[i] + b[i]*x), add=T, col=grau(), xlab='x')
```
The pivoting around 0 is just the mean outcome for the average log population (approx. 22). What's interesting is that model thinks that just above or below the mean the outcome variable either goes to zero or blows up! Nothing in-between.

Something better might be:

```{r}
N <- 100
a <- rnorm(N, 3, 0.5)
b <- rnorm(N, 0, 0.2)
plot(NULL, xlim=c(-2,2), ylim=c(0,100))
for (i in 1:N) curve(exp(a[i] + b[i]*x), add=T, col=grau(), xlab='x')
```
Hmm, a little too tight. Almost on relationship now between log population and tools. 

Let's look at this a different way - instead of the z-score, let's keep population unstandardized:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
par(mfrow=c(1,2))

x_seq <- seq(log(100), log(2e5), length.out=105)
lambda <- vapply(x_seq, \(x) exp(a + b*x), numeric(N))
plot(NULL, xlim=range(x_seq), ylim=range(lambda), xlab='log pop', ylab='total tools')
for (i in 1:N) lines(x_seq, lambda[i,], col=grau(), lwd=1.5)

plot(NULL, xlim=range(exp(x_seq)), ylim=range(lambda), xlab='Population', ylab='total tools')
for (i in 1:N) lines(exp(x_seq), lambda[i,], col=grau(), lwd=1.5)
```

Notice that the left plot we be linear if we logged it. This also shows why it is usually better to log the covariate. We want to imply that there are diminishing returns for adding population at higher values. It means more to add a couple people when the population is very low than compared to when it is high.

We can now go ahead and build our models. We will fit an intercept only model as well as the full model:

```{r}
#| output: false
dat <- list(
  T = d$total_tools,
  P = d$P,
  cid = d$contact_id
)

# intercept only
m11.9 <- ulam(alist(
  T ~ dpois(lambda),
  log(lambda) <- a,
  a ~ dnorm(3, 0.5)
), data=dat, chains=4, cores=4, log_lik = TRUE)

# full model
m11.10 <- ulam(alist(
  T ~ dpois(lambda),
  log(lambda) <- a[cid] + b[cid]*P,
  a[cid] ~ dnorm(3, 0.5),
  b[cid] ~ dnorm(0, 0.2)

), data=dat, chains=4, cores=4, log_lik=TRUE)
```

```{r}
compare(m11.9, m11.10)
```

Notice that model `m11.10` actually has fewer number of effective parameters. This is surprising since we actually have 3 net new parameters in model `m11.10`. But, recall that we can really only think about effective parameters and parameter count relationship in simple linear models. We know that parameter values near boundaries produce less overfitting and therefore contribute less to effective number of parameters. This is true about bounded data too. See that a large amount of the bounds are bounded by population not able to be negative, so introducing this covariate actually reduces the number of effective parameters since it limits the parameter space inadvertently.

Let's go ahead and plot the posterior predictive space:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
par(mfrow=c(1,2))

K <- PSIS(m11.10, pointwise = T)$k
plot(dat$P, dat$T, xlab='log pop (std.)', ylab='tools', col=rangi2, 
     pch=ifelse(dat$cid == 1, 1, 16), lwd=2, cex=1+normalize(K))

ns <- 100
p_seq <- seq(-1.4, max(dat$P), length.out=ns)

for (i in 1:2){
  lambda <- link(m11.10, data=data.frame(P=p_seq, cid=i))
  lmu <- colMeans(lambda)
  lci <- apply(lambda, 2, PI)
  lines(p_seq, lmu, lty=ifelse(i==1,2,1), lwd=1.5)
  shade(lci, p_seq, xpd=T)
}

plot(d$population, d$total_tools, xlab='pop', ylab='tools',
     col=rangi2, pch=ifelse(dat$cid == 1, 1, 16), lwd=2,
     cex=1 + normalize(K))

ns <- 100
p_seq <- seq(-1.4, max(dat$P), length.out=ns)
# Need to convert back to non standard (add mean and multilpy by sd of log pop)
pop_seq <- exp(p_seq*1.53 + 9)

for (i in 1:2){
  lambda <- link(m11.10, data=data.frame(P=p_seq, cid=i))
  lmu <- colMeans(lambda)
  lci <- apply(lambda, 2, PI)
  lines(pop_seq, lmu, lty=ifelse(i==1,2,1), lwd=1.5)
  shade(lci, pop_seq, xpd=T)
}
```

Hawaii is the point in the top right of the right graph. Clearly, because of its very large population, it is highly influential and is therefore contributing to the high $\hat{k}$.

Now, notice that the trend for high contact societies (solid) is higher than that of low contact societies (dashed). But, notice that the model is allowing that past a certain population size, the relationship flips and low contact becomes higher expected than high contact. Of course, a counterfactual of Hawaii with low contact shouldn't be higher than current Hawaii. 

Also, notice that sine we have given the model the intercept as a free parameter, it can be the case where will will have a non-zero number of expected tools when the population is zero. This can never be possible!

