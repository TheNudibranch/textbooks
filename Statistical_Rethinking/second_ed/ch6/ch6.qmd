---
title: "Rethinking - Chapter 6"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true
library(rethinking)
library(dagitty)
```

## 6.0
- collider bias
  - aka: Berkson's Paradox
  - aka: selection-distortion effect
  - Can also happen when adding one predictor into the model induces selection within the model
- In this chapter we will explore:
  1. multicollinearity
  2. post-treatment bias
  3. collider bias


Generate an example of collider bias. Here we use the newsworthiness and trustworthiness example:
```{r}
N <- 200
p <- 0.1
nw <- rnorm(N); tw <- rnorm(N)

# select the top ten 10%
s <- nw + tw
q <- quantile(s, 1-p)
selected <- s >= q
cor(tw[selected], nw[selected])
```
## 6.1 - Multicollinearity

- The raw correlation doesn't matter - the association, conditional on the other variables in the model matters.
- The model will still work fine for prediction, but will fail in understanding

Let's simulate a height dataset, we will then simulate leg length for the right and left leg.
```{r}
set.seed(909)
N <- 100
height <- rnorm(N, 10,2)
leg_prop <- runif(N,0.4,0.5)
leg_left <- leg_prop*height + rnorm(N,0,0.02)
leg_right <- leg_prop*height + rnorm(N,0,0.02)

d <- data.frame(height, leg_left, leg_right)
```

If we regress leg height onto height. We expect the coefficient for leg height to be around 2.2, since the average leg height is 4.5 at the avearge height of 10 (10/4.5).

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

m6.1 <- quap(alist(
  height ~ dnorm(mu, sigma),
  mu <- a + bl*leg_left + br*leg_right,
  a ~ dnorm(10,100),
  bl ~ dnorm(2,10),
  br ~ dnorm(2,10),
  sigma ~ dexp(1)
), data=d)
precis(m6.1)
vcov(m6.1) |> cov2cor()

plot(precis(m6.1))
```

The question we are asking of the model is: "What is the value of knowing each leg's length, after already knowing the other leg's length".

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

par(mfrow=c(1,2))
post <- extract.samples(m6.1)
plot(bl ~ br, post, col=col.alpha(rangi2, 0.1), pch=16)
sum_blr <- post$bl + post$br
dens(sum_blr, col=rangi2, lwd=2, xlab='sum of bl and br')
```

A model with only one of the predictors will produce a coefficient that is inline with the sum of the $\beta_l$ and $\beta_r$

```{r}
m6.2 <- quap(alist(
  height ~ dnorm(mu, sigma),
  mu <- a + bl*leg_left,
  a ~ dnorm(10,100),
  bl ~ dnorm(2,10),
  sigma ~ dexp(1)
), data=d)
precis(m6.2)
```

**Take home message:** When two predictor variables are very strongly correlated (conditional on other variables in the model), including both in a model may lead to confusion.

Let's return to the primate milk example:
```{r}
data(milk)
d <- milk
d$K <- standardize(d$kcal.per.g)
d$fat <- standardize(d$perc.fat)
d$L <- standardize(d$perc.lactose)
```

Let's start with two bivariate regressions
```{r}
m6.3 <- quap(alist(
  K ~ dnorm(mu, sigma),
  mu <- a + bF*fat,
  a ~ dnorm(0,0.2),
  bF ~ dnorm(0,0.5),
  sigma ~ dexp(1)
), data=d)

m6.4 <- quap(alist(
  K ~ dnorm(mu, sigma),
  mu <- a + bL*L,
  a ~ dnorm(0,0.2),
  bL ~ dnorm(0,0.5),
  sigma ~ dexp(1)
), data=d)

precis(m6.3); precis(m6.4)
```
We might think that both are good predictors, but watch what happens when we put both variables into the model:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

m6.5 <- quap(alist(
  K ~ dnorm(mu, sigma),
  mu <- a + bL*L + bF*fat,
  a ~ dnorm(0,0.2),
  bL ~ dnorm(0,0.5),
  bF ~ dnorm(0,0.5),
  sigma ~ dexp(1)
), data=d)

precis(m6.5)
plot(coeftab(m6.3, m6.4, m6.5), pars=c('bF', 'bL'))
```

It is the same problem as the previous leg length example: the two variables form essential a single axis of variation.
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

pairs(~ kcal.per.g + perc.fat + perc.lactose, data=d,col=rangi2,pch=16)
```

Both variables are good predictors on their own, but neither are good predictors if you already know the other. Richie argues that this is because there is a latent variable $D$ that influences both lactose and fat. We are no working with a class of fitting problems known as *non-identifiability*.

Let's simulate some multicollinearity. We are going to create a new column $x$ that is created from $F$ with a correlation parameter.
```{r}
sim.coll <- function(r=0.9){
  d$x <- rnorm(nrow(d), mean=r*d$perc.fat, sd=sqrt((1-r^2)*var(d$perc.fat)))
  m <- lm(kcal.per.g ~ perc.fat + x, data=d)
  sqrt(diag(vcov(m)))[2] #std dev of x
}

rep.sim.coll <- function(r=0.9, n=100){
  stddev <- replicate(n, sim.coll(r))
  mean(stddev)
}

r.seq <- seq(0,0.99,0.01)
stddev <- vapply(r.seq, rep.sim.coll, n=100, numeric(1))

plot(r.seq, stddev, type='l', col=rangi2, lwd=2, xlab='correlation')
```

## 6.2 - Post Treatment Bias

Let's say that we have two soils that we are testing. The goal is for one soil to reduce the growth of fungus (which inhibits plant growth). There are four data points collected: pre plant height, post plant height, treated, and fungus growth. If we were to include fungus growth into the model it would ruin the experiment. Here is a simulation:

```{r}
set.seed(71)
N <- 100

h0 <- rnorm(N,10,2)
treatment <- rep(0:1, each=N/2)
fungus <- rbinom(N, size=1, prob=0.5 - treatment*0.4)
h1 <- h0 + rnorm(N, 5 - 3*fungus)

d <- data.frame(h0,h1,treatment, fungus)
precis(d)
```

Let's define $p=h_{1,i}/h_{0,i}$, the height at time $t=1$ over the height at time $t=0$. We do this since we expect the plant to be taller, but we will let $p$ be less than one just in case things to horribly wrong. We'll use the log-normal to enforce this.

```{r}
m6.6 <- quap(alist(
  h1 ~ dnorm(mu, sigma),
  mu <- h0*p,
  p ~ dlnorm(0,0.25),
  sigma ~ dexp(1)
), data=d)
precis(m6.6)
```
An average about a 40% growth rate. Now, let's include both fungus and treatment into the model:

```{r}
m6.7 <- quap(alist(
  h1 ~ dnorm(mu, sigma),
  mu <- h0*p,
  p <- a + bt*treatment + bf*fungus,
  a ~ dlnorm(0,0.2),
  bt ~ dnorm(0,0.5),
  bf ~ dnorm(0,0.5),
  sigma ~ dexp(1)
), data=d)
precis(m6.7)
```

The heck? Treatment is zero? Yes, because we are effectively asking the question: "once we already know whether or not a plant developed fungus, does soil treatment matter?". The answer of course is no.

Let's build it again, but without fungus:

```{r}
m6.8 <- quap(alist(
  h1 ~ dnorm(mu, sigma),
  mu <- h0*p,
  p <- a + bt*treatment,
  a ~ dlnorm(0,0.2),
  bt ~ dnorm(0,0.5),
  sigma ~ dexp(1)
), data=d)
precis(m6.8)
```

Model `m6.7` is still important though. It tells us that including fungus in the model effectively zeros out the treatment, suggesting that the treatment worked for the anticipated reasons. Let's look at the DAG:

```{r}
plant_dag <- dagitty('dag{H_0 -> H_1; F -> H_1; T -> F}')
coordinates(plant_dag) <- list(x=c(H_0 = 0, T=2, F=1.5, H_1=1), 
                               y=c(H_0=0, T=0, F=0, H_1=0))
drawdag(plant_dag)
```

The correct way of phrasing what happened is: "conditioning on F induces *D-Separation*". Let's look at all the conditional independences:
```{r}
impliedConditionalIndependencies(plant_dag)
```
Notice that the first two don't have any conditions because they are independent *without* any conditionals (colliders).

Let's try a different experiment, where instead of fungus influencing plant growth, it is actually an unobserved variable $M$ moisture. The treatment is also independent of $H_1$. $M$ influences both $F$ and $H_1$. Notice, if we condition on fungus we open up the path and $T$ can now influence $H_1$, even though $T$ is indep. of $H_1$ without any conditionals.

```{r}
N <- 100
h0 <- rnorm(N,10,2)
treatment <- rep(0:1,each=N/2)
M <- rbern(N)
fungus <- rbinom(N, size=1, prob=0.5 - treatment*0.4 + 0.4*M)
h1 <- h0 + rnorm(N, 5 + 3*M)
d2 <- data.frame(h0, h1, treatment, fungus)

m6.7.1 <- quap(alist(
  h1 ~ dnorm(mu, sigma),
  mu <- h0*p,
  p <- a + bt*treatment + bf*fungus,
  a ~ dlnorm(0,0.2),
  bt ~ dnorm(0,0.5),
  bf ~ dnorm(0,0.5),
  sigma ~ dexp(1)
), data=d2)
precis(m6.7.1)

m6.8.1 <- quap(alist(
  h1 ~ dnorm(mu, sigma),
  mu <- h0*p,
  p <- a + bt*treatment,
  a ~ dlnorm(0,0.2),
  bt ~ dnorm(0,0.5),
  sigma ~ dexp(1)
), data=d2)
precis(m6.8.1)

```

## 6.3 - Collider Bias

A variable is a collider if there are arrows going into it.

Let's simulate happiness and age. These variables are not associated with each other, but they both influence marriage.
```{r}
# I'm not sure why we need to simulate it for 1,000 years
# It looks like for N_years >= 65 it will always return 65*20 =1,300 rows
# Which makes sense, but why run it for so long if the return is just a snapshot of the current population
# After 1,000 years (i.e. no history)
d <- sim_happiness(seed=1977, N_years = 1e3)
precis(d)
```
Let's build a simple regression model to determine the effect of age on happiness while controlling for marriage status. We'll also only focus on adults here. We're going to build our prior model such that happiness is at its peak at age 18 and lowest at age 65. Variable $A$ will encode this by ranging from 1 to 0.

```{r}
d2 <- d[d$age > 17,]
d2$A <- (d2$age - 18)/(65-18)
d2$mid <- d2$married + 1
```

Since happiness ranges from -2 to 2, the strongest association for age would be a coef. of 4 since that would take us from the min to max (if that was true the intercept would probably be -2 to make it all work out).

```{r}
m6.9 <- quap(alist(
  happiness ~ dnorm(mu, sigma),
  mu <- a[mid] + bA*A,
  a[mid] ~ dnorm(0,1),
  bA ~ dnorm(0,2),
  sigma ~ dexp(1)
), data=d2)
precis(m6.9, depth=2)
```

Strong relationship between age and happiness. Let's see what happens when we remove marriage:
```{r}
m6.10 <- quap(alist(
  happiness ~ dnorm(mu, sigma),
  mu <- a + bA*A,
  a ~ dnorm(0,1),
  bA ~ dnorm(0,2),
  sigma ~ dexp(1)
), data=d2)
precis(m6.10)
```
Woah! No association found, marriage is a collider!


Grandparent, Parent, Child example:

If we add the unobserved U, it will bias inference from $G \rightarrow C$. This is becuase if we condition on $P$ and regress $G$ on $C$, then we don't only get $G\rightarrow C$, but we also now get $G\rightarrow P \leftarrow U \rightarrow C$, because $P$ now becomes a collider. 

Let's do some simulations to prove this:
```{r}
N <- 200
b_GP <- 1 # direct effect of G on P
b_GC <- 0 # direct effect of G on C
b_PC <- 1 # direct effect of P on C
b_U <- 2  # direct effect of U on P and C

set.seed(1)
U <- 2*rbern(N,0.5) - 1
G <- rnorm(N)
P <- rnorm(N, b_GP*G + b_U*U)
C <- rnorm(N, b_PC*P + b_GC*G + b_U*U)
d <- data.frame(C,P,G,U)

m6.11 <- quap(alist(
  C ~ dnorm(mu, sigma),
  mu <- a + b_PC*P + b_GC*G,
  a ~ dnorm(0,1),
  c(b_PC, b_GC) ~ dnorm(0,1),
  sigma ~ dexp(1)
), data=d)
precis(m6.11)
```

Two things going on here. We have simple confounding for $\beta_{PC}$ because it is not measuring the direct effect. We also have the confounding effect of $U$, causing $\beta_{PC}$ to be higher than expected. Second, we have the collider effect causing $\beta_{GC}$ to have a significant magnitude, when it should be zero.

This is pretty cool when you think about it, that is, why are we seeing $\beta_{PC}$ as negative. Well, we know that a bad neighborhood ($U=-1$) hurts child and parent education. But, lets say that we condition on parent education. That is, we have some sub-population of kids $s$ that have parents with the same education level. Now, what happens when we look at grandparent education. There will be some that are high and some that are low. But, we have conditioned on parent education, therefore the changes in grandparent education can't effect parent education. What can? Neighborhood status. This is how we get differing grandparent education, with the same parent education - neighborhood status controls this. Thus, if we have grandparent $G_1$ with a high education and grandparent $G_2$ with a low education, chances are $P_1$ is in a bad neighborhood and $P_2$ is in a good neighborhood to keep $P_1 = P_2$. Thus, $C_1$ will be lower and $C_2$ will be higher. A negative correlation.

Let's now add U into the model for completeness.
```{r}
m6.12 <- quap(alist(
  C ~ dnorm(mu, sigma),
  mu <- a + b_PC*P + b_GC*G + b_U*U,
  a ~ dnorm(0,1),
  c(b_PC, b_GC, b_U) ~ dnorm(0,1),
  sigma ~ dexp(1)
), data=d)
precis(m6.12)
```

