---
title: "Rethinking - Chapter 12"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true
library(rethinking)
library(dagitty)
library(tidyverse)
```

## 12.0

## 12.1 - Over-dispersed counts

When counts are more variable than a pure process, the exhibit **over-dispersion**. This implies that the variance is much larger than the mean. This usually implies that we are omitting a variable, and that is causing additional dispersion in the model.

As a rule of thumb, it is usually better to use multilevel models instead, but these models are still useful. Multilevel models can handle over-dispersion and other kinds of heterogeneity.

### Beta binomial

A beta-binomial arises when we model the probability of success as its own distribution for each observation. For example, the `UCBadmit` data set was over-dispersed if we don't account for the variation between departments.

We use a beta distribution for each row in the data set.That is, each row gets its own unobserved probability distribution. We use a Beta because it is relatively easy to marginalize out the beta distribution and make the likelihood easier which we will see later.

We will use a different parameterization than usual for the beta. Here we have $(\bar{p}, \sigma)$ where $\bar p$ is the average probability and $\theta$ is the scale.

```{r}
pbar <- 0.5
theta <- 5
curve(dbeta2(x, pbar, theta), 0, 1)
```

When $\theta = 2$ then every probability is equally likely (for when $\bar p = 0.5$). When $\theta < 2$, the distribution is so dispersed that extreme probabilities near zero and 1 are more likely than the mean.

This will be our model:

\begin{align}
A_i &\sim \text{BetaBinomial}(N_i, \bar{p}_i, \theta) \\
\text{logit}(\bar{p}_i) &= \alpha_{\text{GID[i]}} \\
\alpha_j &\sim \text{Normal}(0, 1.5)\\
\theta &= \phi + 2 \\
\phi &\sim \text{Exponential}(1)
\end{align}

where $A$ is `admit`, $N$ is the number of applications, and GID is the gender index. Since we don't want the probability values to be piling up near 0 and 1, we make it so that the lower bound of $\theta$ is 2.

```{r}
#| output: false

data("UCBadmit")
d <- UCBadmit
d$gid <- as.integer(d$applicant.gender == 'female') + 1
dat <- list(A=d$admit, N=d$applications, gid=d$gid)

m12.1 <- ulam(alist(
  A ~ dbetabinom(N, pbar, theta),
  logit(pbar) <- a[gid],
  a[gid] ~ dnorm(0, 1.5),
  transpars> theta <<- phi + 2.0,
  phi ~ dexp(1)
), data=dat, chains=4)
```
Note that we are employing a "tag" `transpars>` so that Stan will tag the transformation in the "transformed variables" block. This is so Stan will return it in the samples. We don't actually need to do the Jacobian adjustment here.

```{r}
post <- extract.samples(m12.1)
post$da <- post$a[,1] - post$a[,2]
precis(post, depth=2)
```

Look at `da` the difference in log-odds between the male and female effects. We see that we are uncertain about the difference and cannot rule out zero. This is different from the naive conclusion that we came to in the last chapter before we took into accounting the confounding variables. The difference here is that we are implicitly giving an intercept term to *every row* in the data set through the use of the beta binomial model.

To see this let's look at a quick sim:

```{r}
gid <- 2
curve(dbeta2(x, mean(logistic(post$a[,gid])), mean(post$theta)), 0,1, ylab='', 
      xlab='prob admit', lwd=2, ylim=c(0,3))

for (i in 1:50){
  p <- logistic(post$a[i, gid])
  theta <- post$theta[i]
  curve(dbeta2(x, p, theta), add=T, col=col.alpha('black', 0.2))
}
```

Notice that the plausible distribution allows for departments to admit almost all applicants. We have allowed for over-dispersion!

Let's see how it did at an observation level:

```{r}
postcheck(m12.1)
```

The model doesn't know about departments, but it does see the heterogeneity and accounts for it. 

### Negative-binomial or gamma-Poisson

The negative-binomial is more usefully called the gamma-Poisson. Similarly to the Beta-Binomial, we replace the mean parameter with a distribution itself. In this case, it is a gamma distribution. Predictor variables adjust the shape (and mean) of this distribution, not the expected value of each observation.

The Gamma-Poisson distribution is parameterized as such:

$$y_i \sim \text{Gamma-Poisson}(\lambda_i, \phi)$$

where $\lambda$ can be treated as the rate of the usual Poisson. The $\phi$ must be positive and controls the variance. In turn, the variance of the Gamma-Poisson is $\lambda + \lambda^2/\phi$. Notice that for larger values of $\phi$ it converges to a usual Poisson.

Recall the Oceanic example from the previous chapter. There, Hawaii pulled the mean. Here, we expect much less since the Gamma-Poisson expects more variation.

```{r}
#| output: false

data("Kline")
d <- Kline
d$P <- standardize(log(d$population))
d$contact_id <- ifelse(d$contact == 'high', 2L, 1L)

dat2 <- list(T = d$total_tools, P=d$population, cid=d$contact_id)

m12.1 <- ulam(alist(
  T ~ dgampois(lambda, phi),
  lambda <- exp(a[cid])* P^b[cid] / g,
  a[cid] ~ dnorm(1,1),
  b[cid] ~ dexp(1),
  g ~ dexp(1),
  phi ~ dexp(1)
), data=dat2, chains=4, log_lik=T)
```

Recall that the $\lambda$ function here was informed by the literature.

```{r}
plot(d$population, d$total_tools, xlab='pop', ylab='tools',
     col=rangi2, pch=ifelse(dat2$cid == 1, 1, 16), lwd=2, ylim=c(0,100))

ns <- 100
p_seq <- seq(0, max(dat2$P), length.out=ns)

for (i in 1:2){
  lambda <- link(m12.1, data=data.frame(P=p_seq, cid=i))
  lmu <- colMeans(lambda)
  lci <- apply(lambda, 2, PI)
  lines(p_seq, lmu, lty=ifelse(i==1,2,1), lwd=1.5)
  shade(lci, p_seq, xpd=T)
}
```

Its hard to see from the graph alone, but if we compare to the model from ch 11, we see that the Hawaii exerts a lot less influence over the regression line.

### Over-dispersion, entropy, and information criteria

The beta-binomial and gamma-poisson are still maximum entropy for the same constraints as the regular binomial and poisson.

Richie argues against using WAIC and PSIS here since we are modeling on aggregated data. It is true that the reason we get a nice response for the admission data is because we are allowing the model to vary by row. *But*, we could still compare this to a aggregated regular binomial model. We just couldn't compare it to a dis-aggregated binomial model, but we already knew that. I think Richie is just arguing that if we were to instead dis-aggregate to the row level, we would lose the reason we built the beta-binomial model in the first place.

He also states that we will solve for this in the next chapter.

Just a quick overview: this all works because we are marginalzing out the parameter of interest. That is, the parameter/expected value that we were calculating in the previous chapter. Essential we have:

$$f(y|n, \bar p, \theta) = \int_0^1 g(y|n,p) h(p|\bar p, \theta) dp$$

where $g$ is the binomial and $h$ is the beta distribution. We are marginalizing out the very parameter that we were modeling in the previous chapter.

## 12.2 - Zero-inflated outcomes

Mixture model: model that uses more than one likelihood for the same outcomes variable. In this case, we will use for the different ways in which a zero can manifest.

Going back to the monk and manuscript problem, let's assume that zero manuscripts produced can occur in one of two ways. 1) they spent the day drinking, 2) they legitimately didn't get any manuscripts done that day. Let $p$ be the probability that they spent the day drinking and $\lambda$ be the mean number of manuscripts completed when the monks work.

The likelihood for zero is then:

\begin{align}
P(0|p, \lambda) &= P(\text{drink}|p) + P(\text{work}|p)\cdot P(0| \lambda) \\
&= p + (1-p) \exp (-\lambda)
\end{align}

Note $P(0|\lambda) = \exp(-\lambda)$ since $P(y|\lambda) = \lambda^y \exp(-\lambda)/y!$.

It's relatively simple to show:

$$p(y|y>0,\lambda)=(1-p) \frac{\lambda^y \exp(-\lambda)}{y!}$$

Since we know that they probability of drinking is zero since they produced more than zero manuscripts.

The $(1-p)$ is necessary to conserve total probability. For instance, if we sum over all possible values of $y$ we must get 1. Now, if we didn't have the $(1-p)$ term in front of the likelihood for the $y>0$ case, we would be able to have a total probability greater than one if we sum of all $y$. 

Define the ZIPoisson as follows:

\begin{align}
y_i &\sim \text{ZIPoisson}(p_i, \lambda_i) \\ 
\text{logit}(p_i) &= \alpha_p + \beta_p x_i \\ 
\log(\lambda_i) &= \alpha_\lambda + \beta_\lambda x_i
\end{align}

Let's simulate some data:

```{r}
prob_drink <- 0.2
rate_work <- 1

N <- 365

drink <- rbinom(N, 1, prob_drink)
y <- (1-drink)*rpois(N, rate_work)

simplehist(y)
zero_drinks <- sum(drink)
zero_work <- sum(y==0 & drink == 0)
zero_total <- sum(y==0)
lines(c(0,0), c(zero_work, zero_total), lwd=4, col=rangi2)
```
Recall that we don't actually get to see the # drink days, but we can see just how much they overwhelm the final distribution.

Let's fit the model:

```{r}
#| output: false

m12.3 <- ulam(alist(
  y ~ dzipois(p, lambda),
  logit(p) <- ap,
  log(lambda) <- al,
  ap ~ dnorm(-1.5, 1),
  al ~ dnorm(1, 0.5)
), data=list(y=y), chains=4)
```

```{r}
precis(m12.3)
```

Convert back to the natural scale:
```{r}
post <- extract.samples(m12.3)
mean(inv_logit(post$ap))
mean(exp(post$al))
```


Notice we get a nice estimate for the proportion of days they drink, but we are unable to say *which* days the monks drank.

## 12.3 Ordered categorical outcomes

- For likert data, it is not the same to move a person from a 1 to a 2, that is from a 5 to a 6. 
  - Those jumps might be different 
- To model this we'll use a cumulative link function

For a little philosophy, consider the following principles:

- The action principle: harm caused by an action is morally worse than equivalent harm caused by omission
- The intention principle: Harm intended as the means to a goal is morally worse than equivalent harm foreseen as the side effect of a goal
- The contact principle: Using physical contact to cause harm to a victim is morally worse than causing equivalent harm to a victim without using physical contact 

Consider a version of the trolley problem that contains the action and intention principle. For this, it is the usual "pull the lever to save 5, but you kill one". But in this version, pulling the lever causes the one person to fall onto the tracks in front of the five. Therefore slowing down the train (the intention principle).

Let's look at some data where we see how people respond to this sort of dilemma.

```{r}
data("Trolley")
d <- Trolley
```

In the data set there are 331 unique individuals. Responses are integers from 1 to 7, indicating how morally permissible the action was.

```{r}
simplehist(d$response, xlim=c(1,7), xlab='response')
```

Here we will be looking at the cumulative log odds. The reason we use the log odds is so we can translate our constrain 0-1 scale to the full reals:


```{r}
p_cum <- (table(d$response)/nrow(d)) %>% cumsum()
logit <- \(x) log(x / (1-x))
log_p_cum <- logit(p_cum)
par(mfrow=c(1,2))
plot(1:7, p_cum, type='b', main='Cuml P')
plot(1:7, log_p_cum, type='b', main='log odds cuml p')
```
To model this, we need an intercept for every unique outcome $k$

$$\log \frac{p(y_i\leq k)}{1-p(y_i\leq k)} = \alpha_k$$

```{r}
log_p_cum |> round(2)
```

Notice that since the total probability for the last category is always one, we don't have to model a specific intercept for it. So we only have $K-1$ free parameters.

For the likelihood of the data, we have:

$$p_k = p(y_i=k)=p(y_i\leq k)-p(y_i\leq k-1)$$

We can write the total model as:

\begin{align}
R_i &\sim \text{Ordered-logit}(\phi_i, \kappa) \\
\phi_i &= 0 \\
\kappa_k &\sim \text{Normal}(0,1.5)
\end{align}

We can define it in terms of the categorical likelihood only as:


\begin{align}
R_i &\sim \text{Categorical}(\textbf{p}) \\
p_1 &= q_1 \\
p_k &= q_k - q_{k-1} \hspace{5mm} \text{for} K>k>1 \\
p_K &= 1 - q_{k-1} \\
\text{logit}(q_k) &= \kappa_k - \phi_i \\
\phi_i &= \text{terms of linear model} \\
\kappa_k &\sim \text{Normal}(0, 1.5)
\end{align}

Notice that the Ordered-logit is really just a Categorical distribution takes a probability vector for the different responses. Where the response probability has been derived from the cumulative log odds. Also, for this instance we have $\kappa$ as the intercept vector.

Also, in the original model, we set $\phi = 0$. This encodes that there is no observation level terms - all explanatory power comes from the intercept terms.

Let's go ahead and fit this model:

```{r}
#| output: false

m12.4 <- ulam(alist(
  R ~ dordlogit(0, cutpoints),
  cutpoints ~ dnorm(0, 1.5)
), data=list(R=d$response), chains=4, cores=4, iter=300)
```


You can still use `quap`, but you need to give initial values:

```{r}
init_list <- lapply(-2:2, I) %>% append(2.5)
names(init_list) <- paste0('a', 1:6)

m12.4q <- quap(alist(
  response ~ dordlogit(0, c(a1, a2, a3, a4, a5, a6)),
  cutpoints ~ dnorm(0, 1.5)
), data=d, start=init_list)
```

Let's look at the posterior for `m12.4`. Recall this is on the cumulative log-odds scale:

```{r}
precis(m12.4)
```

We can recover the cumulative probabilities by:

```{r}
coef(m12.4) |> inv_logit() |> round(3)
```
These are the same values that we computed earlier, but now with a uncertainty interval.


### Adding predictors

Now we let $\phi_i = \beta x_i$. Notice from our definition above that our log cumulative odds is $\kappa_k - \phi_i$.

Here is something counter intuitive - we are subtracting $\phi_i$ and that is actually *increasing* out output. A quick example:

Here is the individual probabilities:
```{r}
pk <- dordlogit(1:7, 0, coef(m12.4))
round(pk,2)
```

If we weight them by the possible outcomes, we get the average response:

```{r}
sum(1:7*pk)
```
Now, let's subtract 0.5 from the coefs before calculating `pk`:

```{r}
pk <- dordlogit(1:7, 0, coef(m12.4) - 0.5)
sum(1:7*pk)
```

It actually went up! This is because the linear term $\phi$ is not changing the underlying latent distribution, but instead the cutpoints. Let's see what is actually happening under the hood. Also, technically the latent distribution is a logistic distribution, but it is easier to code up the normal:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
#| 
par(mfrow=c(1,2))
curve(dnorm, -4, 4, lwd=3)
title('Original Cutpoint position')
cut_vec <- c(-8, unlist(init_list), 8)
for (i in 1:(length(init_list) + 1)) {
  s <- seq(cut_vec[i], cut_vec[i+1], length.out=100)
  polygon(x=c(s, rev(s)), y=c(rep(0,length(s)), dnorm(rev(s))), col=i)
  abline(v=cut_vec[[i+1]], lwd=2, lty=2)
}
curve(dnorm, -4, 4, lwd=3, add=T)

new_cuts <- unlist(init_list) - 1.5
curve(dnorm, -4, 4, lwd=3)
title('New Cutpoints')
cut_vec <- c(-8, new_cuts, 8)
for (i in 1:(length(new_cuts) + 1)) {
  s <- seq(cut_vec[i], cut_vec[i+1], length.out=100)
  polygon(x=c(s, rev(s)), y=c(rep(0,length(s)), dnorm(rev(s))), col=i)
  abline(v=new_cuts[i+1], lwd=2, lty=2)
}
curve(dnorm, -4, 4, lwd=3, add=T)
```

Notice that by shifting the cutpoints down, we are actually allocating more probability to the higher categories.

Looking back at our trolley data, we need to encode the information about the various principles. One key feature to note is that the contact principle, necessarily requires the action principle. With this we have the following cases:

- No Action, contact, or intention
- action
- contact
- intention
- action and intention
- contact and intention

We don't have an interaction between action and contact because two coefficients (with intercepts for 0 case) is enough to encode all the possible (3) cases.

The equation we will use looks like:

\begin{align}
\log \frac{p(y_i \leq k)}{1 - p(y_i \leq k)} &= \alpha_k - \phi_i \\
\phi_i &= \beta_A A_i + \beta_C C_i + B_{I,i} I_i \\
B_{I, i} &= \beta_I + \beta_{IA} A_i + \beta_{IC} C_i
\end{align}

Now let's fit the model:

```{r}
#| output: false
dat <- list(
  R = d$response,
  A = d$action,
  I = d$intention,
  C = d$contact
)

m12.5 <- ulam(alist(
  R ~ dordlogit(phi, cutpoints),
  phi <- bA*A + bC*C + BI*I,
  BI <- bI + bIA*A + bIC*C,
  c(bA, bC, bI, bIA, bIC) ~ dnorm(0,0.5),
  cutpoints ~ dnorm(0,1.5)
), data=dat, chains=4, cores=4, iter=300)
```

```{r}
precis(m12.5)
```

```{r}
plot(precis(m12.5))
```

Notice that all the coefficients are negative - meaning they reduce the given rating when present.

Theses models are a bit hard to plot since the outcome for a single point is a $K$ length vector. One way we can attempt to is by visualizing the cumulative probability as it changes for each outcome:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6


plot(NULL, type='n', xlab='intention', ylab='probability',
     xlim=c(0,1), ylim=c(0,1), xaxp=c(0,1,1), yaxp=c(0,1,2), yaxt='n')

seq(0,1,0.2) %>% {axis(side=2, at=., labels=.); abline(h=., col='lightgrey')}

kA <- kC <- 0
kI <- 0:1
pdat <- data.frame(A=kA, C=kC, I=kI)
phi <- link(m12.5, dat=pdat)$phi

post <- extract.samples(m12.5)
for (s in 1:50){
  pk <- pordlogit(1:6, phi[s,], post$cutpoints[s,])
  for (i in 1:6){
    lines(kI, pk[,i], col=grau(0.1))
  }
}
```

Where `pordlogit` gives us the cumulative probability for each of the categories. Here is the last one:

```{r}
pk
```

Let's see what a simulated histogram looks like so we can see the different implied counts change:

```{r}
# Im not sure why this isn't working - my guess is that the new version of 
# Stan broke something
# s <- sim(m12.5, data=pdat)
```

## 12.4 Ordered Categorical predictors

Let's look and recode the education level from the trolley problem:

```{r}
levels(d$edu)
edu_levels <- c(6, 1, 8, 4, 7, 2, 5, 3)
d$edu_new <- edu_levels[d$edu] # only works if d$edu is a factor
```

To encode this predictor, define a vector of length $c - 1$ where $c$ is the number of unique categories. This vector $\delta$ will be defined to sum to one. We can also define some coefficient $\beta_E$ which is the value of reaching the highest education - a graduate degree. We only need $c-1$ length since one of the categories will get absorbed into the intercept. This makes our definition of $\phi$:

$$\phi_i = \beta_E \sum_{j=0}^{E_i-1} \delta_j + \text{other stuff}$$

where $E_i$ is the ordered category for education completed. For $E_i=1$, that is "Elementary School", we let $\delta_0=0$. Thus when $E_i=1$, $\beta_E$ is no longer in the model.

Let's now show the full model with our new ordinal predictor:

\begin{align}
R_i &\sim \text{Ordered-logit}(\phi_i, \kappa) \\ \\
\phi_i &= \beta_E \sum_{j=0}^{E_i-1} \delta_j + \beta_A A_i + \beta_I I_i + \beta_C C_i \\
\kappa_k &\sim \text{Normal}(0, 1.5) \\
\beta_A, \beta_I, \beta_C, \beta_E &\sim \text{Normal}(0,1) \\
\delta &\sim \text{Dirichlet}(\alpha)
\end{align}

Notice the introduction of the new Dirichlet distribution. This is the multivariate extension of the beta and accepts a $\alpha$ vector as the parameter where the length of $\alpha$ is the number of unique categories.

Let's simulate some draws from a Dirichlet:

```{r}
library(gtools)
delta <- rdirichlet(10, alpha=rep(2,7))

h <- 3
plot(NULL, xlim=c(1,7), ylim=c(0,0.5), ylab='prob')
for (i in 1:nrow(delta)){
  lines(1:7, delta[i,], type='b', pch=ifelse(i==h, 16, 1), lwd=ifelse(i==h, 4, 1.5),
        col=ifelse(i==h, 'black', col.alpha('black', 0.7)))
}
```

Let's see what this model looks like:

```{r}
# dat <- list(
#   R = d$response,
#   action = d$action,
#   intention = d$intention,
#   contact = d$contact,
#   E = as.integer(d$edu_new),
#   alpha=rep(2, 7)
# )
# 
# m12.6 <- ulam(alist(
#   R ~ ordered_logistic(phi, kappa),
#   phi <- bE*sum(delta_j[1:E]) + bA*action + bC*contact + bI*intention,
#   kappa ~ normal(0, 1.5),
#   c(bA, bI, bC, bE) ~ normal(0,1),
#   vector[8]: delta_j <<- append_row(0, delta),
#   simplex[7]: delta ~ dirichlet(alpha)
# ), data=dat, chains=4, cores=4, iter=200, start=list(kappa=seq(-3,3,length.out=6)))

```


To handle the $\delta_0=0$ case we used Stan syntax explicitly. We defined defined a `vector` object that manually placed a 0 in the first position. 