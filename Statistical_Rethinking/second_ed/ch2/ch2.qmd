---
title: "Rethinking - Chapter 2"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true
library(rethinking)
```


## 2.0
- Savage (Jimmy) is credited with the notion that models are *small worlds*, are representations of only a small part of reality.
- Small world versus large world
  - Model is most always a representations of a subset of reality (small world)
  - Allows for things such as optimality, but we are not guaranteed this in the real world
    - Consider data drift, concept drift, etc.

## 2.2 - Building a model
- Many different stories correspond to the same model
  - This is what we were talking about with the previous chapter. There can be multiple data generating process that give rise to the same model
- Updating steps can be done backwards instead of forwards
  - This is why frequentist tests are not really objective
  - We can always work backwards and get what their implicit prior was
- You are not done when the model is fit
  - Need to interrogate the model
  - Ask questions that are fringe and or check understanding
  - Posterior retrodictive checks

## 2.3 - Components of the model
- Bayesian priors are no more subjective than the likelihoods or repeated sampling assumption of classical significance testing
- Priors should be interrogated

## 2.4 - Making the model go
- Grid Approximation
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

p_grid <- seq(0,1,length.out=50) # grid for y
prior <- rep(1,length(p_grid)) #flat prior
like <- dbinom(6, size=9, prob=p_grid)

post_uns <- like*prior
post <- post_uns/sum(post_uns)

par(mfrow=c(1,2))
plot(p_grid, prior, type='b', xlab='prob of water', ylab='prior prob')
plot(p_grid, post, type='b', xlab='prob of water', ylab='posterior prob')
```

- Quadratic approximation
  - Using gradients to find the MAP
  - At the MAP, uses estimates of the curvature to find the parameters of the normal approximation
    - It's Hessian times boys
```{r}
global_qa <- quap(
  alist(
    W ~ dbinom(W+L, p),
    p ~ dunif(0,1)
  ),
  data = list(W=6, L=3)
)
precis(global_qa)
```
- Assuming that the posterior is Gaussian, it is maximized at 0.67 and has a standard deviation of 0.16.


- Now, let's assume that we are using conjugacey, where the prior distribution is $\text{Beta}(1,1)$. The conjugate posterior with a binomial likelihood would be $\text{Beta}(W+1, L+1)$.
  - For more info: https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

W <- 6
L <- 3
curve(dbeta(x, W+1, L+1), 0, 1, lwd=2)
curve(dnorm(x, 0.67, 0.16), col='darkblue', lwd=2, add=T)
legend('topleft', legend=c('Conjugate', 'Quadratic Approx.'), col=c(1, 'darkblue'), lwd=2)
```

- Quadratic approximation can be sometimes equivalent to the estimate obtained by MLE.
- log of Gaussian only needs the second derivative since it is a parabola

- Simple metropolis hastings algorithm
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

n_samp <- 1e3
p <- rep(NA, n_samp)
p[1] <- 0.5

W <- 6; L <- 3
for (i in 2:n_samp){
  p_new <- rnorm(1, p[i-1], 0.1) |> abs() # proposal
  if (p_new > 1) p_new <- p_new - 1
  q0 <- dbinom(W, W+L, p[i-1])
  q1 <- dbinom(W, W+L, p_new)
  p[i] <- ifelse(runif(1) < q1/q0, p_new, p[i-1])
}
par(mfrow=c(1,2))
hist(p)
plot(p, type='l', lwd=2)
```

Let's see how well it did:
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
#| code-fold: true

dens_p <- density(p)
x_seq <- seq(0,1,length.out=1e3)
conj <-  x_seq |> dbeta(W+1, L+1)
plot(dens_p, ylim=range(conj, dens_p$y), lwd=3, col='darkred', main='', xlab='p')
lines(x_seq, conj, col='darkblue', lwd=3)
legend('topleft', legend=c('Metropolis Hastings', 'Conjugate'), col=c('darkred', 'darkblue'), lwd=2)
```

## Exercises
### E.2
- (1)

### M.1
- Using a uniform prior, compute posterior with the following data
  1. W,W,W
  2. W,W,W,L
  3. L,W,W,L,W,W

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

get_post <- function(grid, n_W, n_L){
  prior <- rep(1,length(grid)) #flat prior
  like <- dbinom(n_W, size=n_L+n_W, prob=grid)
  post_uns <- like*prior
  like*prior / sum(like*prior)
}

p_grid <- seq(0,1,length.out=50) # grid for y

plot(p_grid, get_post(p_grid, 3,0), type='b', xlab='prob of water', ylab='posterior prob', col='darkblue')
lines(p_grid, get_post(p_grid, 3,1), type='b', xlab='prob of water', ylab='posterior prob', col='darkgreen')
lines(p_grid, get_post(p_grid, 4,2), type='b', xlab='prob of water', ylab='posterior prob', col='darkred')
legend('topleft', legend=c('Data 1', 'Data 2', 'Data 3'), col=c('darkblue', 'darkgreen', 'darkred'), lwd=2)
```

### M.2
- Assume a prior of $p=0$ when $p< 0.5$
- Doesn't matter what constant is used since it all gets washed out in the end
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6
#| code-fold: true

get_post <- function(grid, n_W, n_L){
  prior <- rep(1,length(grid)) #flat prior
  prior[grid < 0.5] <- 0
  like <- dbinom(n_W, size=n_L+n_W, prob=grid)
  post_uns <- like*prior
  like*prior / sum(like*prior)
}

p_grid <- seq(0,1,length.out=50) # grid for y

plot(p_grid, get_post(p_grid, 3,0), type='b', xlab='prob of water', ylab='posterior prob', col='darkblue')
lines(p_grid, get_post(p_grid, 3,1), type='b', xlab='prob of water', ylab='posterior prob', col='darkgreen')
lines(p_grid, get_post(p_grid, 4,2), type='b', xlab='prob of water', ylab='posterior prob', col='darkred')
legend('topleft', legend=c('Data 1', 'Data 2', 'Data 3'), col=c('darkblue', 'darkgreen', 'darkred'), lwd=2)
```
### M.3
- From the problem we know that $P(\text{Land} | \text{Earth}) = 0.3$ and $P(\text{Land} | \text{Mars}) = 1$.
- Prior of 0.5 for both having been tossed
- Therefore $P(\text{Land}) = 0.5\cdot P(\text{Land} | \text{Earth}) + 0.5\cdot P(\text{Land} | \text{Mars}) = 0.65$
- Note
\begin{align}
P(\text{Earth}|\text{Land}) &= \frac{P(\text{Land} | \text{Earth})\cdot P(\text{Earth})}{P(\text{Land})} \\
&= \frac{0.3\cdot 0.5}{0.65} \\
&= 0.23
\end{align}


### H.1
- Pandas bears, both equally likely in the wild
- $P(t|A) = 0.1$, $P(t|B) = 0.2$
- Find $P(t_2|t_1)$
- $P(t_1,t_2) = P(t_2|t_1) * P(t_1)$
- We have $p(t_1) = 0.5 * P(t_1|a) + 0.5 * P(t_1|b) = 0.5*0.1 + 0.5*0.2=0.15$
- $p(t_1, t_2) = 0.5 * P(t_1,t_2|a) + 0.5 * P(t_1,t_2|b) = 0.5*0.1^2 + 0.5*0.2^2 = 0.025$
- Therefore: $P(t_2|t_1) = 0.025/0.15 = 0.1667$

### H.2
- Find $p(a|t)$
- Some logic as M.3
- $p(t) = 0.15$, $p(a) = 0.5$, $p(t|a) = 0.1$
- Therefore: $p(a|t) = p(t|a)*p(a)/p(t) = 0.1*0.5/0.15 = 0.333$

### H.3
- Just use previous posterior as prior, that is $p(a)^\prime=p(a|t)$
- $p(a|t_1, \neg t_2) = p(\neg t_2|a) * p(a)^\prime/p(\neg t_2)^\prime$
- **You can't just do** $p(t) = 0.15$, then have $p(\neg t) = 0.85$,
  - This is because we have the prior information
  - It is really $p(\neg t)^\prime = p(\neg t_2|t_1)$, but we can still do the averging with the pervious info
  - $p(\neg t)^\prime = p(a)^\prime*p(\neg t) + p(b)^\prime*p(\neg t) = 0.33*0.9 + 0.67*0.8 = 0.84$
- and from our initial information, $p(t|a) = 0.1$, therefore $p(\neg t|a) = 0.9$
- Thus, $p(a|t_1, \neg t_2) = p(\neg t_2|a) * p(a)^\prime/p(\neg t_2)^\prime = 0.9*0.333/0.84 = 0.36$


