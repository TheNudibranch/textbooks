---
title: "Rethinking - Chapter 9"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true
library(rethinking)
library(dagitty)
library(tidyverse)
```

## 9.0

## 9.1 - Good King Markov and his island kingdom

Short description of metropolis hastings:

  - island rings of 10 islands, each larger than the last by a factor of $\frac{\text{current island number}}{\text{prev island number}}$
  - This would mean that island 3 is 3 times as big as island 1
  - king wants to visits all islands in proportion to their population
  - flip a coin, if heads go clockwise, else counterclockwise
  - Pick up $n$ shells equal to the proposed islands number
  - Pick up $m$ stones equal to the current islands number
  - If $n > m$, always choose proposed
  - Else, subtract $n$ from $m$ to get $l = m - n$ stones. Place the new $l$ stones and $n$ shells in a bag
  - Draw from the bag
  - If shell, go to proposal, else stay
  - Probability of **staying** is equal to $n/m$

If we were on island 3 and proposed island 2, the probability of staying should be higher (2/3). If we want the prob of leaving, we have 1 minus that, or 1/3. Thus, odds of 1 to 2 ($\frac{p}{1-p}$), so we have 2 stones and 1 shell in the bag.

I'm honestly not a fan of this explanation.

Here is a little sim:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

num_weeks <- 1e5
positions <- rep(0, num_weeks)
current <- 10
for (i in 1:num_weeks){
  positions[i] <- current
  proposal <- current + sample(c(-1,1), size=1)
  
  if (proposal < 1) proposal <- 10
  if (proposal > 10) proposal <- 1
  
  prob_move <- proposal/current
  current <- ifelse(runif(1) < prob_move, proposal, current)
}
table(positions) |> plot()
```

Notice that we don't need to know the entire population of islands. Just the current and proposal. This is the beauty of mcmc.

## 9.2 - Metropolis algorithms

One of the key elements for Metropolis is the symmetric proposal. Meaning that there is an equal chance of proposing A to B as B to A. Whether we transition between states or stay is of course a different story.

Now, Metropolis is just the general algorithm that requires symmetric proposals. There is the Metropolis Hastings algorithm which does not require symmetry. This is helpful for parameters like standard deviations where we can't go below zero.

Gibbs sampling is a special case of Metropolis Hastings, where we make *adaptive proposals*. Meaning that our proposals change depending on where we are in the distribution. One drawback of Gibbs is that we have to have conjugate priors. They also get stuck in high dimensional spaces since parameter spaces with many dimensions tend to have regions with high correlation.

One universal truth is that any MCMC approach that samples individual parameters in individual steps will get stuck once the number of params. grows sufficiently large.

One reason for this is that as we have higher dimension parameter spaces, the probability mass doesn't concentrate around the mode of the parameter space. We could see this with a simple 10 dimension gaussian. Let's draw some random samples from the distribution and see how far they are from the mode - should be at the origin:

```{r}
D <- 10
Y <- rmvnorm(1e4, rep(0,D), diag(D))
Rd <- Y |> apply(1, \(x) sqrt(sum(x^2)))
dens(Rd)
```

This is why we need samplers that focus on the entire distribution. 

## 9.3 - Hamiltonian Monte Carlo

We have another king. His kingdom is in a valley running north/south where population is inversely proportional to elevation. Here is his (HMC) algorithm for choosing where to visit. Notice how we are not restricted for visiting only neighboring states.

- Pick a direction: north or south
- Start the vehicle with a random momentum
- The vehicle will obviously fall down due to gravity, but it will also go back up the other side some amount
- After some random amount of time has passed, stop the vehicle

Notice how we wouldn't have been able to use this for the discrete island approach.

Let's do a toy model to help. Assume that we have 100 $x$ and 100 $y$ values, all sampled from Normal(0,1). We'll use the following model:

\begin{align}
x_i &\sim \text{Normal}(\mu_x, 1) \\ 
y_i &\sim \text{Normal}(\mu_y, 1) \\ 
\mu_x &\sim \text{Normal}(0, 0.5) \\ 
\mu_y &\sim \text{Normal}(0, 0.5) 
\end{align}

HMC requires two functions: a log posterior to tell us our "elevation" and the gradient to let us know the steepness of the mountain. As well as two settings: the number of leapfrog steps and step size.

We need to provide 5 things to run HMC:

- A function `U` that returns the negative log-probability of the data at the current position
- A function `grad_U` that returns the gradient of the negative log-probability
- A step size $\epsilon$
- A count of leapfrog steps `L`
- A starting position `current_q`

```{r}
U <- function(q, a=0, b=1, k=0, d=1){
  muy <- q[1]; mux <- q[2]
  U <- sum(dnorm(y, muy, 1, log=T)) + sum(dnorm(x, mux, 1, log=T)) + 
    dnorm(muy, a, b, log=T) + dnorm(mux, k, d, log=T)
  -U
}
```


For the graidents, recall that

$$\frac{\partial \log \text{N}(y|a,b)}{\partial a} = \frac{y - a}{b^2}$$

Therefore:

$$\frac{\partial U}{\partial \mu_x} = \frac{\partial \log\text{N}(x|\mu_x,1)}{\partial \mu_x} + \frac{\partial \log\text{N}(\mu_x|0,0.5)}{\partial \mu_x} = \sum_i \frac{x_i - \mu_x}{1^2} + \frac{0 - \mu_x}{0.5^2}$$

```{r}
U_grad <- function(q, a=0, b=1, k=0, d=1){
  muy <- q[1]; mux <- q[2]
  G1 <- sum(y - muy) + (a - muy)/b^2
  G2 <- sum(x - mux) + (k - mux)/d^2
  
  c(-G1, -G2)
}

x <- rnorm(50) |> scale() |> as.numeric() 
y <- rnorm(50) |> scale() |> as.numeric()
```

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

library(shape)
Q <- list()
Q$q <- c(-0.1, 0.2)
pr <- 0.5
plot(NULL, ylab='muy', xlab='mux', xlim=c(-pr,pr), ylim=c(-pr,pr))
step <- 0.03

L <- 11 # 0.03/28 for U-turns
n_samples <- 4
path_col <- col.alpha('black', 0.5)

points(Q$q[1], Q$q[2], pch=4)

for (i in 1:n_samples){
  Q <- HMC2(U, U_grad, step, L, Q$q)
  if (n_samples < 10){
    for (j in 1:L){
      K0 <- sum(Q$ptraj[j,]^2)/2 # Kinetic Energy
      lines(Q$traj[j:(j+1), 1], Q$traj[j:(j+1), 2], col=path_col, lwd=1+2*K0)
    }
    points(Q$traj[1:L+1, ], pch=16, col='white', cex=0.35)
    Arrows(Q$traj[L,1], Q$traj[L,2], Q$traj[L+1, 1], Q$traj[L+1, 2], 
           arr.length = 0.35, arr.adj = 0.7) 
    text(Q$traj[L+1, 1], Q$traj[L+1, 2], i, cex=0.8, pos=4, offset=0.4)
  }
  points(Q$traj[L+1, 1], Q$traj[L+1, 2], pch=ifelse(Q$accept==1, 16,1),
         col=ifelse(abs(Q$dH) > 0.1, 'red', 'black'))
}
```


Let's take a closer look at the `HMC2` function:

```{r}
HMC2_copy <- function(U, U_grad, epsilon, L, current_q){
  q <- current_q
  p <- rnorm(length(q), 0, 1) # random momentum
  current_p <- p
  # Make a half step for momentum
  p <- p - epsilon * U_grad(q)/2
  qtraj <- matrix(NA, nrow=L+1, ncol=length(q))
  ptraj <- qtraj
  qtraj[1,] <- current_q
  p_traj[1,] <- p
  
  # Now we need to do our leapfrog steps
  for (i in 1:L){
    q <- q + epsilon * p # Full step for the position
    # Make a full step for the momentum, except for at the end of trajectory
    if (i != L){
      p <- p - epsilon * grad_U(q)
      ptraj[i+1,] <- p
    }
    qtraj[i+1, ] <- q
  }
  # Make half step for momentum at the end
  p <- p - epsilon * grad_U(q)/2
  ptraj[L+1,] <- p
  
  # Negate the momentum at end of traj to make proposal symmetric
  p <- -p
  
  # Evaluate potential and kinetic energies at start and end of traj
  current_U <- U(current_q)
  current_K <- sum(current_p^2)/2
  proposed_U <- U(q)
  proposed_K <- sum(p^2)/2
  
  accept <- 0
  if(runif(1) < exp(current_U - proposed_U + current_K - proposed_K)){
    new_q <- q # accept
    accept <- 1
  }
  else new_q <- current_q # reject
  list(q=new_q, traj=qtraj, ptraj=ptraj, accept=accept)
}
```

Notice how we check if we should accept or reject at the end. This is to see if the energy at the end of the trajectory is significantly different than where it was at the beginning. If this is the case, we have a *divergent transition* and must reject.

Remember that HMC is not magic - we can't sample discrete parameter spaces. But, we can get around that by marginalizing out the discrete parameters when we sample.

## 9.4 - Easy HMC: `ulam`

To use `ulam` we:

- need to preprocess all variable transforms
  - For example, if the outcome variable is log transformed, do that before writing the forumula
  - If not, this will be done every step of the MCMC sampler
- construct a clean variable list with only the variables we will use
  - Stan won't sample if we have any `NA` values

Let's go back to the terrain ruggedness from ch. 8.

```{r}
data(rugged)
d <- rugged
d$log_gdp <- log(d$rgdppc_2000)
dd <- d[complete.cases(d$rgdppc_2000), ]
dd$log_gdp_std <- dd$log_gdp / mean(dd$log_gdp)
dd$rugged_std <- dd$rugged / max(dd$rugged)
dd$cid <- ifelse(dd$cont_africa == 1, 1, 2)
```

Recall the way we fit the interaction model with `quap`

```{r}
m8.3 <- quap(alist(
  log_gdp_std ~ dnorm(mu, sigma),
  mu <- a[cid] + b[cid] * (rugged_std - 0.215),
  a[cid] ~ dnorm(1, 0.1),
  b[cid] ~ dnorm(0, 0.3),
  sigma ~ dexp(1)
), data=dd)
precis(m8.3, dept=2)
```
We've already pre-transformed our variables, so now we just need a slimmed down list:

```{r}
dat_slim <- list(
  log_gdp_std = dd$log_gdp_std,
  rugged_std = dd$rugged_std,
  cid = as.integer(dd$cid)
)
str(dat_slim)
```

It is better to use a list since a list can have data of varying lengths. This will come in handy when we have multilevel models in later chapters.

```{r}
m9.1 <- ulam(alist(
  log_gdp_std ~ dnorm(mu, sigma),
  mu <- a[cid] + b[cid] * (rugged_std - 0.215),
  a[cid] ~ dnorm(1, 0.1),
  b[cid] ~ dnorm(0, 0.3),
  sigma ~ dexp(1)
), chains=1, data=dat_slim)
```

```{r}
precis(m9.1, depth=2)
```


Here we see $N_{\text{eff}}$ and $\hat{R}$. The first is a crude metric to determine the number of independent samples. The latter is a measure of convergence. For the effective sample size, it has been shown that as little as 200 is good enough to do summary quantities.

Let's now go up to 4 cores:


```{r}
#| output: false

m9.1 <- ulam(alist(
  log_gdp_std ~ dnorm(mu, sigma),
  mu <- a[cid] + b[cid] * (rugged_std - 0.215),
  a[cid] ~ dnorm(1, 0.1),
  b[cid] ~ dnorm(0, 0.3),
  sigma ~ dexp(1)
), chains=4, cores=4, data=dat_slim)
```

We can also use the `show` function if we forget the model specification:

```{r}
show(m9.1)
```

Okay, let's look at the output with the multiple chains:

```{r}
precis(m9.1, depth=2)
```

Notice that the number of effective samples is actually greater than the number of samples themselves ($S = \frac{1000}{2}\cdot 4 =2000$). This is becauase HMC does so well we actually have anti-correlation.

Let's take a look at the pairs plot:

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

pairs(m9.1)
```

The upper right is the scatter of the two parameters in question. The lower left is the respective correlations.

For more checks, let's look at the trace plots:


```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

traceplot(m9.1)
```

This is cool because the warmup is included. You can see how `sigma` started off high, but eventually found its place. This is also called the *adaptation phase*. These samples are automatically discarded when we run `extract.samples`.

Let's look at another view of this: the trank plot:


```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

trankplot(m9.1)
```

Above is a histogram showing the rank of each samples amongst each chain. It is a little cleaner and shows that our chains have mixed well. The x-axis is rank, from 1 to the number of samples across all chains (2,000). Notice that this means we are ranking across chains. The y-axis is the frequency of ranks in each bin. 

Also, you can always inspect the stan code by using:

```{r}
stancode(m9.1)
```

## 9.5 - Care and feeding of your Markov Chains


