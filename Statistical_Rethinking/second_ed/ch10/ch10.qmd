---
title: "Rethinking - Chapter 10"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true
library(rethinking)
library(dagitty)
library(tidyverse)
```

## 10.0

- Go for the distribution with the *largest entropy*
  - Is the least informative, while being the most true to the information we provide
  - Nature tends to produce distributions with high entropy
    - Recall the Gaussian distribution that arises from repeated addition
  - It just works and has been shown to repeatedly 

The posterior distribution is also a great example of maximum entropy. It is the distribution that has the smallest divergence from the prior while still remaining consistent with the data and constraints.


## 10.1 - Maximum Entropy

Recall that maximum was seen in ch 7 following the three deserata:

- Measure is continuous
- Increase as the number of possible events increases
- Should be additive

We get the following measure of uncertainty (information entropy) for distribution $p$ for probabilities $p_i$ for events $i$.

$$H(p) = - \sum_i p_i \log p_i$$

 Let's think about an example where we have 10 pebbles and 5 buckets. We want to get the entropy of each configuration where it is equally likely to get a pebble in each of the buckets.
 
```{r}
p <- list()
p$A <- c(0,0,10,0,0)
p$B <- c(0,1,8,1,0)
p$C <- c(0,2,6,2,0)
p$D <- c(1,2,4,2,1)
p$E <- c(2,2,2,2,2)

p_norm <- lapply(p, \(x) x/sum(x))

H <- lapply(p_norm, \(x) -sum(ifelse(x==0, 0, x*log(x)))) |> unlist()
H
```
 Notice that distribution E, which can be realized by the greatest number of ways, has the highest entropy.
 
```{r}
ways <- c(1, 90, 1260, 37800, 113400)
log_ways <- log(ways)
plot(log_ways, H)
```
 
Note, information entropy is an approximation of the log ways per pebble. Again, showing that entropy is the measure of the number of unique arrangements. As the number of pebbles grows larger, the approximation gets better.

Recall from ch 2 that a Gaussian emerges from an ensemble of small factors adding together. The Gaussian is the best choice of a distribution when variance is known (or is known to be finite). Let' introduce the *generalized normal distribution*

$$\text{Pr}(y|\mu,\alpha, \beta) = \frac{\beta}{2 \alpha \Gamma(1/\beta)} e^{-(\frac{|y-\mu|}{\alpha})^\beta}$$

The goal is the compare a regular Gaussian to several generalized normals with the same variance. See the book for the plot, but we see that entropy is maximized for $\beta=2$ and variance set to $\sigma^2=1$. What we see is that the regular Gaussian is as spread out as possible while also maintaining the $\sigma^2$ constraint. That's it. No other constraint then constant variance. Now, when we feel comfortable making additional assumptions, that is when the principle of maximum entropy leads to a different distribution.

See book for a really nice derivation for why Gaussian is maximum entropy distribution when we only constrain the variance.

Now, lets show that the binomial distribution is the maximum entropy distribution when:

- Only two un-ordered events
- constant expected values
 
Let's say that in two trials of picking a marble with a even amount of white and blue, we expect to draw 1.4 blue marbles. This is equivalent to a $p=0.7$. The entropy of the binomial distribution with this expected value is:

```{r}
p <- 0.7
# ww, bw, wb, bb
A <- c((1-p)^2, p*(1-p), (1-p)*p, p^2)
-sum(A*log(A))
```

Now, let's simulate some other distributions that aren't the binomial:

```{r}
sim_p <- function(G=1.4){
  x123 <- runif(3)
  x4 <- (G * sum(x123) - x123[2] - x123[3]) / (2-G)
  z <- sum(x123, x4)
  p <- c(x123, x4) / z
  list(H=-sum(p*log(p)), p=p)
}
a <- sim_p()
a
sum(a$p * c(0,1,1,2))
```

Let's simulate 100,000 distributions:

```{r}
H <- replicate(1e5, sim_p())
dens(as.numeric(H[1,]), adj=0.1)
abline(v=-sum(A*log(A)), lty=2)
```
We see that A, the distribution generated by the binomial, is the maximum entropy. Let's see which of our simulated distributions got the closest:

```{r}
entropies <- as.numeric(H[1,])
H[2,][which.max(entropies)]
A
```
Practically identical to A! Something else to notice, A is as close as we can get to even while also maintaining the required expected value.

 


