---
title: "Rethinking - Chapter 7"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true
library(rethinking)
library(dagitty)
```

## 7.0

- We are worried about
  - Overfitting
  - Underfitting
  - Confounding
- We will see that confounding, while incorrect, can produce more *accurate* models than if we had removed the confounding variable
  - Most accurate is not synonymous with causally correct
  - We must ask if we want to understand causes, or just predict
- **regularizing prior**: tell the model to not get too excited about new data
- predictor variables that improve prediction are not always statistically significant
  - The inverse can be true as well - stat sig variables that don't help prediction
  - They are two different tasks
- AIC uses MAP estimates instead of the entire posterior
  - It also requires flat priors
- The BIC is not actually an "information criterion"
- WAIC can provide the same results as AIC when assumptions are met
  - AIC is therefore a special case of WAIC - a purely Bayesian information criteria

## 7.1 - Problems with parameters

Introduce $R^2$:

$$R^2 = \frac{\text{var(outcome) - var(residuals)}}{\text{var(outcome)}} = 1 - \frac{\text{var(residuals)}}{\text{var(outcome)}}$$
The $R^2$ will also increase the more predictors we add, even when the variables added are just random noise.

- Regular features: target features because they generalize well

Let's look at a dataset of average brain volumes and body mass
```{r}
sppnames <- c('afarensis', 'africanus', 'habilis', 'boisei', 'rudolfensis', 'ergaster', 'sapiens')
brainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)
masskg <- c(37, 35.5, 34.5, 41.5, 55.5, 61, 53.5)
d <- data.frame(species=sppnames, brain=brainvolcc, mass=masskg)
```

Let's rescale and center body mass. We will just rescale brain to be between 0 and 1 since we want to keep the zero reference point. We'll also build a simple linear model.

```{r}
d$mass_std <- standardize(d$mass)
d$brain_std <- d$brain / max(d$brain)
```


The simple linear model as absurd priors, but that is part of the lesson:
```{r}
m7.1 <- quap(alist(
  brain_std ~ dnorm(mu, exp(log_sigma)),
  mu <- a + b*mass_std,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10),
  log_sigma ~ dnorm(0,1) # Bakes out the same as log normal
), data=d)
```

Just as an aside, you can extract samples from a simple OLS model, since OLS is really just an approximate bayes algorithm with flat priors. You won't get a sigma posterior though

```{r}
m7.1_OLS <- lm(brain_std ~ mass_std, data=d)
post <- extract.samples(m7.1_OLS)
```

Let's calculate $R^2$ for this model. Here we actually want the average squared deviation since we are in a Bayesian context. We don't want the $n-1$ correction term. This is implemented in Richie's `var2` function

```{r}
var2
```

Alright, let's compute:

```{r}
set.seed(12)

R2_is_bad <- function(quap_fit){
  s <- sim(quap_fit)
  r <- colMeans(s) - d$brain_std
  resid_var <- var2(r)
  outcome_var <- var2(d$brain_std)
  1 - resid_var/outcome_var  
}

R2_is_bad(m7.1)
```

Alright, let's go crazy with polynomials:

```{r}
#| code-fold: true

m7.2 <- quap(alist(
  brain_std ~ dnorm(mu, exp(log_sigma)),
  mu <- a + b[1]*mass_std + b[2]*mass_std^2,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10),
  log_sigma ~ dnorm(0,1) # Bakes out the same as log normal
), data=d, start=list(b=rep(0,2)))

m7.3 <- quap(alist(
  brain_std ~ dnorm(mu, exp(log_sigma)),
  mu <- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10),
  log_sigma ~ dnorm(0,1) # Bakes out the same as log normal
), data=d, start=list(b=rep(0,3)))

m7.4 <- quap(alist(
  brain_std ~ dnorm(mu, exp(log_sigma)),
  mu <- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10),
  log_sigma ~ dnorm(0,1) # Bakes out the same as log normal
), data=d, start=list(b=rep(0,4)))

m7.5 <- quap(alist(
  brain_std ~ dnorm(mu, exp(log_sigma)),
  mu <- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4 + b[5]*mass_std^5,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10),
  log_sigma ~ dnorm(0,1) # Bakes out the same as log normal
), data=d, start=list(b=rep(0,5)))

m7.6 <- quap(alist(
  brain_std ~ dnorm(mu, 0.001),
  mu <- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4 + b[5]*mass_std^5 + b[5]*mass_std^6,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10)
), data=d, start=list(b=rep(0,5)))
```

The last model `m7.6` has its variance replace with 0.001. I'm guessing this is because if we didn't hard code it we would have 8 parameters for 7 points of data.

Let's go ahead and plot these:
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

plot_func <- function(quap_fit, name){
  post <- extract.samples(quap_fit)
  mass_seq <- seq(min(d$mass_std), max(d$mass_std), length.out=100)
  l <- link(quap_fit, data=list(mass_std=mass_seq))
  mu <- colMeans(l)
  ci <- apply(l, 2, PI)
  plot(brain_std ~ mass_std, data=d, main=paste0('Model ', name, 'R^2=', round(R2_is_bad(quap_fit), 2)), ylim=c(range(ci, mu, d$brain_std)))
  lines(mass_seq, mu)
  shade(ci, mass_seq)
}

par(mfrow=c(2,3))
for(i in 1:6) paste0('plot_func(m7.', i,',',i, ')') |> parse(text=_) |> eval()

```

Notice that the model predicts negative brain size for `m7.6`, not great. Model `m7.1` could be said to be underfit. This means that it is not sensitive to changes in the sample. If we removed a point, it would probably look relatively the same. Alternatively, model `m7.4` is overfit. If we remove a point, the polynomial mean line would look wildly different.

Bias-variance: bias (underfitting) and variance (overfitting). 