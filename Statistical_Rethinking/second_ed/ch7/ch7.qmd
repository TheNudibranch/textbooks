---
title: "Rethinking - Chapter 7"
format: 
  html:
    code-fold: show
toc: true
---
```{r}
#| output: false
#| code-fold: true
library(rethinking)
library(dagitty)
```

## 7.0

- We are worried about
  - Overfitting
  - Underfitting
  - Confounding
- We will see that confounding, while incorrect, can produce more *accurate* models than if we had removed the confounding variable
  - Most accurate is not synonymous with causally correct
  - We must ask if we want to understand causes, or just predict
- **regularizing prior**: tell the model to not get too excited about new data
- predictor variables that improve prediction are not always statistically significant
  - The inverse can be true as well - stat sig variables that don't help prediction
  - They are two different tasks
- AIC uses MAP estimates instead of the entire posterior
  - It also requires flat priors
- The BIC is not actually an "information criterion"
- WAIC can provide the same results as AIC when assumptions are met
  - AIC is therefore a special case of WAIC - a purely Bayesian information criteria

## 7.1 - Problems with parameters

Introduce $R^2$:

$$R^2 = \frac{\text{var(outcome) - var(residuals)}}{\text{var(outcome)}} = 1 - \frac{\text{var(residuals)}}{\text{var(outcome)}}$$
The $R^2$ will also increase the more predictors we add, even when the variables added are just random noise.

- Regular features: target features because they generalize well

Let's look at a dataset of average brain volumes and body mass
```{r}
sppnames <- c('afarensis', 'africanus', 'habilis', 'boisei', 'rudolfensis', 'ergaster', 'sapiens')
brainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)
masskg <- c(37, 35.5, 34.5, 41.5, 55.5, 61, 53.5)
d <- data.frame(species=sppnames, brain=brainvolcc, mass=masskg)
```

Let's rescale and center body mass. We will just rescale brain to be between 0 and 1 since we want to keep the zero reference point. We'll also build a simple linear model.

```{r}
d$mass_std <- standardize(d$mass)
d$brain_std <- d$brain / max(d$brain)
```


The simple linear model as absurd priors, but that is part of the lesson:
```{r}
m7.1 <- quap(alist(
  brain_std ~ dnorm(mu, exp(log_sigma)),
  mu <- a + b*mass_std,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10),
  log_sigma ~ dnorm(0,1) # Bakes out the same as log normal
), data=d)
```

Just as an aside, you can extract samples from a simple OLS model, since OLS is really just an approximate bayes algorithm with flat priors. You won't get a sigma posterior though

```{r}
m7.1_OLS <- lm(brain_std ~ mass_std, data=d)
post <- extract.samples(m7.1_OLS)
```

Let's calculate $R^2$ for this model. Here we actually want the average squared deviation since we are in a Bayesian context. We don't want the $n-1$ correction term. This is implemented in Richie's `var2` function

```{r}
var2
```

Alright, let's compute:

```{r}
set.seed(12)

R2_is_bad <- function(quap_fit){
  s <- sim(quap_fit)
  r <- colMeans(s) - d$brain_std
  resid_var <- var2(r)
  outcome_var <- var2(d$brain_std)
  1 - resid_var/outcome_var  
}

R2_is_bad(m7.1)
```

Alright, let's go crazy with polynomials:

```{r}
#| code-fold: true

m7.2 <- quap(alist(
  brain_std ~ dnorm(mu, exp(log_sigma)),
  mu <- a + b[1]*mass_std + b[2]*mass_std^2,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10),
  log_sigma ~ dnorm(0,1) # Bakes out the same as log normal
), data=d, start=list(b=rep(0,2)))

m7.3 <- quap(alist(
  brain_std ~ dnorm(mu, exp(log_sigma)),
  mu <- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10),
  log_sigma ~ dnorm(0,1) # Bakes out the same as log normal
), data=d, start=list(b=rep(0,3)))

m7.4 <- quap(alist(
  brain_std ~ dnorm(mu, exp(log_sigma)),
  mu <- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10),
  log_sigma ~ dnorm(0,1) # Bakes out the same as log normal
), data=d, start=list(b=rep(0,4)))

m7.5 <- quap(alist(
  brain_std ~ dnorm(mu, exp(log_sigma)),
  mu <- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4 + b[5]*mass_std^5,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10),
  log_sigma ~ dnorm(0,1) # Bakes out the same as log normal
), data=d, start=list(b=rep(0,5)))

m7.6 <- quap(alist(
  brain_std ~ dnorm(mu, 0.001),
  mu <- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4 + b[5]*mass_std^5 + b[5]*mass_std^6,
  a ~ dnorm(0.5, 1),
  b ~ dnorm(0, 10)
), data=d, start=list(b=rep(0,5)))
```

The last model `m7.6` has its variance replace with 0.001. I'm guessing this is because if we didn't hard code it we would have 8 parameters for 7 points of data.

Let's go ahead and plot these:
```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 6

plot_func <- function(quap_fit, name){
  post <- extract.samples(quap_fit)
  mass_seq <- seq(min(d$mass_std), max(d$mass_std), length.out=100)
  l <- link(quap_fit, data=list(mass_std=mass_seq))
  mu <- colMeans(l)
  ci <- apply(l, 2, PI)
  plot(brain_std ~ mass_std, data=d, main=paste0('Model ', name, 'R^2=', round(R2_is_bad(quap_fit), 2)), ylim=c(range(ci, mu, d$brain_std)))
  lines(mass_seq, mu)
  shade(ci, mass_seq)
}

par(mfrow=c(2,3))
for(i in 1:6) paste0('plot_func(m7.', i,',',i, ')') |> parse(text=_) |> eval()

```

Notice that the model predicts negative brain size for `m7.6`, not great. Model `m7.1` could be said to be underfit. This means that it is not sensitive to changes in the sample. If we removed a point, it would probably look relatively the same. Alternatively, model `m7.4` is overfit. If we remove a point, the polynomial mean line would look wildly different.

Bias-variance: bias (underfitting) and variance (overfitting). 

## 7.2 - Entropy and accuracy

We need to:

- Establish a measurement scale for distance from perfect accuracy
- Need to measure *deviance* as an approximation of relative distance from perfect accuracy
- Establish that it is *deviance* out-of-sample that is only of interest

To define a target, there are two main dimensions to worry about:

- cost-benefit analysis
- Accuracy in context

Joint probability is the measure we want. It's the unique measure that correctly counts up the relative number of ways each event could happen. The true model will also have the highest joint probability.

**Information:** The reduction in uncertainty when we learn an outcome.

What is a good measure of uncertainty:

- A measure of uncertainty should be continuous
- The measure should increase as the number of possible events increases
- Measures should be additive. 

The function that satisfies these desiderata has one form and is called **information entropy**. For $n$ possible events, with each event $i$ occurring with probability $p_i$, then the entropy is:

$$H(p)=-E_p[\log(p_i)]=-\sum^n_{i=1}p_i\log(p_i)$$

We can use entropy to build measures of accuracy. Also, when $p_i=0$, we can use L'Hopitals rule to show $p_i\log(p_i)=0$. Notice, also that maximizing entropy is the same as decreasing uncertainty. 

**Divergence:** The additional uncertainty induced by using probabilites from one distribution to describe anoter distribution. 

The KL divergence is defined as:

$$D_{\text{KL}}(p,q)=\sum_i p_i \log(p_i/q_i)$$

In plain english: the average difference in log probability between the target $p$ and model $q$. This is also just the difference between two entropies: the entropy of the target distribution $p$ and the *cross entropy* arising from using $q$ to predict $p$. We can use this metric to predict different models.

One thing to callout here is the definition of cross entropy:

$$H(p,q)=-E\log(p_i)=-\sum^n_{i=1}p_i\log(q_i)$$
This is really just the expected value of $\log(q)$ with respect to $p$. This also shows why KL Divergence is not symmetric - you are changing the reference distribution for the cross entropy.  

Notice that we almost never have $p$, the true distribution. But, we have some estimates $q$ and $r$, we can compare these two since $p$ will drop out in the comparison. It is convention to just take the sum of the log-probabilities. There is a convenient function to do just this:

```{r}
set.seed(1)
lppd(m7.1,n=1e4)
```
If you summed the above, you'd get the log-probability score. Larger values are better since they indicate larger average accuracy. It is common to see this called deviance, but multiplied by -2. the -2 is for historical reasons and is really done so the sample distribution falls more in line with a Chi-squared distribution.

The `lppd` function shown above is called the **log-pointwise-predictive-density** as is defined as:

$$\text{lppd}(y,\Theta)=\sum_i \log\frac{1}{S}\sum_s p(y_i|\Theta_s)$$

Where $S$ is the number of samples and $\Theta_s$ is the s-th sampled parameter vector from the posterior.

Here is the code to do just that:

```{r}
set.seed(1)
logprob <- sim(m7.1, ll=T, n=1e4) # Same function used in ch4, will return 
  # a matrix of log-probability values of ll=T
ns <- nrow(logprob)
n <- ncol(logprob)
f <- \(x) log_sum_exp(logprob[,x]) - log(ns)
vapply(1:n, f, numeric(1))
```

We are still going to suffer from the same issue that the $R^2$ had: the scores will get better as the model becomes more complex.

```{r}
set.seed(1)
vapply(1:6, \(x){
  paste0('lppd(m7.',x,') |> sum()') |> parse(text=_) |> eval()
}, numeric(1))
```

```{r}
logprob <- sim(m7.6, ll=T, n=1e4) # Same function used in ch4, will return 
  # a matrix of log-probability values of ll=T
ns <- nrow(logprob)
n <- ncol(logprob)
f <- \(x) log_sum_exp(logprob[,x]) - log(ns)
vapply(1:n, f, numeric(1))

precis(m7.6, depth=2)
```

