---
title: ROS Chapter 16 - Design and Sample Size Decisions
output: 
  html_document:
    code_folding: hide
---
```{r, echo=F}
knitr::opts_chunk$set(cache = T)
```
# 16.2 General principles of design

- In designing a study, it is generally better the double the effect size $\theta$ rather than double the sample size $n$. This is because standard errors decrease with the *square root* of sample size, not the full magnitude (or half if we are saying double).

### Sample size to acheive a specified probability of obtaining stat sig
Assume that we have some test of proportions. We wish to determine if $p > 1/2$ based on the estimate $\hat{p} = y/n$. Using a conservative s.e. of $\sqrt{0.5\cdot 0.5/n}$ (since for any binomial dist, the max variance is when $p=0.5$), the 95% conf interval is given by $[\hat{p}\pm 1.9\cdot 0.5 / \sqrt{n}]$, where 1.96 is 97.5% quantile of the standard normal. If our comparison point is 0.5, then we would have shown stat sig if $\hat{p} > 0.5 + 1.96\cdot 0.5/\sqrt{n}$. Consider the following null distribution for $H_0: p = 0.5$, with the area outside the 95% confidence interval shaded purple..

```{r, fig.align='center', fig.width=10, fig.height=6, warning=FALSE, message=FALSE}
library(magrittr)
library(rstanarm)
se <- 0.5/sqrt(196)
x <- seq(0.2, 0.8, length.out=1e3)
d <- dnorm(x, mean=0.5, sd=se)
plot(x, d, lwd=4, type='l', col='darkblue')

crit_value <- qnorm(0.975, 0.5, se)

polygon(c(x[x>crit_value], rev(x[x > crit_value])),
        (x > crit_value) %>% {c(d[.], rep(0,sum(.)))},
        col=adjustcolor('purple', 0.7), border=NA)

crit_value <- qnorm(0.025, 0.5, se)
polygon(c(x[x<crit_value], rev(x[x < crit_value])),
        (x < crit_value) %>% {c(d[.], rep(0,sum(.)))},
        col=adjustcolor('purple', 0.7), border=NA)
lines(x, d, lwd=4, type='l', col='darkblue')
```
Now, if we want to find the power, we must assume some effect size. For instance, $p=0.6$. It might be tempting to test whether $p = 0.6 > 0.5 + 1.96 \cdot  0.5/\sqrt{n}$ and then find a sample size from this formula, but this is incorrect. The assumption is that $p=0.6$, which in turn makes the standard error $\sqrt{0.6\cdot 0.4/n}$. So how would one find the right sample size - we need to specify the power of the test. Notice the null $H_0: p=0.5$ and alternative $H_A:p=0.6$ plotted below:

```{r, fig.align='center', fig.width=10, fig.height=6}
x <- seq(0.2, 0.8, length.out=1e3)
d1 <- dnorm(x, mean=0.5, sd=se)
d2 <- dnorm(x, mean=0.6, sd=se)
plot(x, d, lwd=4, type='l', col='darkblue')
lines(x, d2, lwd=4, type='l', col='darkred')
abline(v=0.5, lty=2, col='darkblue', lwd=3)
abline(v=0.6, lty=2, col='darkred', lwd=3)
```
Power is the defined as the probability of that we will correctly reject the null when the alternative is true. This is the probability that $\hat{p} > 0.5 + 1.96\cdot se$, given that $p=0.6$. The shaded region below shows this:

```{r, fig.align='center', fig.width=10, fig.height=6}
x <- seq(0.2, 0.8, length.out=1e3)
d1 <- dnorm(x, mean=0.5, sd=se)
d2 <- dnorm(x, mean=0.6, sd=se)
plot(x, d, lwd=4, type='l', col='darkblue')
crit_value <- qnorm(0.975, 0.5, se)

polygon(c(x[x>crit_value], rev(x[x > crit_value])),
        (x > crit_value) %>% {c(d2[.], rep(0,sum(.)))},
        col=adjustcolor('purple', 0.7), border=NA)
lines(x, d2, lwd=4, type='l', col='darkred')
abline(v=0.5, lty=2, col='darkblue', lwd=3)
abline(v=0.6, lty=2, col='darkred', lwd=3)

```

Notice, we can also find the start of the rejection region using a given power value. If we set power at 80%, then the start of the rejection region is given by $0.6 - \text{qnorm(0.2)}\cdot se = 0.6+0.84\cdot se$. Thus, we can set the two equations for the start of the rejection region equal and obtain $$0.5 + 1.96\cdot se = 0.6 + 0.84\cdot se$$

Recall that $se=\sqrt{0.4*0.6/n}$ since $p=0.6$ is the assumption if the alternative is true:

$$
\begin{align}
0.5 + 1.96\cdot se &= 0.6 - 0.84\cdot se \\
2.8\cdot se &= 0.1 \\
\end{align}
$$
Just stopping here because this is a big result and is used as a rule of thumb all the time. If we want 80% at 5% significance level, the effect size must be greater than or equal to 2.8 standard errors.

A bit more algebra shows that $n = (0.49 \cdot 2.8/0.1)^2$.


# 16.4
### You need 4 times the sample size to estimate an interaction that is the same size as the main effect
- If a study is designed to have 80% power to detect a main effect at a 95% confidence interval, then that implies that the true effect size is 2.8 s.e. away from zero.

```{r class.source = 'fold-show'}
pnorm(2.8, 1.96, 1)
```
- Suppose that the interaction of interest is the same size as the main effect
- If the average treatment effect on the entire population is $\theta$
  - Assume an effect of $0.5\theta$ among women and $1.5\theta$ among men
  - Then the interaction - the difference in treatment effect comparing men to women - is the same size as the main effect. 
- The standard error of the interaction is roughly *twice* the standard error of the main effect:

\newline

- Assume constant variance between groups (should be good for binary data) and even split in sample size
- The difference between treatment and control $\bar{y}_T - \bar{y}_C$, has a standard error of $\sqrt{\sigma^2/(n/2) + \sigma^2/(n/2)} = 2\sigma/\sqrt{n}$
- The estimate of the interaction $(\bar{y}_{T,\text{men}} - \bar{y}_{C,\text{men}}) - (\bar{y}_{T,\text{women}} - \bar{y}_{C,\text{women}})$ has a standard error of $\sqrt{\sigma^2/(n/4) + \sigma^2/(n/4) + \sigma^2/(n/4) + \sigma^2/(n/4)} = 4\sigma/\sqrt{n}$
- If we have set the sample size such that $2\sigma/\sqrt{n}$ the minimum standard error for 80% and 95% CI, then we need 4 times $n^\prime=4n$ the sample size such that 

\begin{align}
\frac{4\sigma}{\sqrt{n^\prime}} &= \frac{4\sigma}{\sqrt{4n}}\\
&= \frac{4\sigma}{2\sqrt{n}}\\
&= \frac{2\sigma}{\sqrt{n}}
\end{align}

So what would be the power of your estimate if we assumed that the main effect and study was set up in such a way that we achieved 80% at 95% CI? Well, we would still only reject if the z-score is above 1.96, and we know that the effect size on the standardized scale is 2.8 for 80%, then the interaction would be at 1.4 (this is if we didn't perform the $4n$ correction above.

```{r class.source = 'fold-show'}
# Show two different methods
pnorm(1.4, 1.96, 1) |> round(2)

rnorm(1e6, 1.4) %>% `>`(1.96) %>% mean() %>% round(2) %>% print()
```
We can also use the simulation for the second method to show the problem of the "winners curse". If we condition on significance, what order of magnitude will we over estimate the effect?

```{r class.source = 'fold-show'}
raw <- rnorm(1e6, 1.4, 1)
signif <- raw > 1.96
mean(raw[signif]) |> round(2)
```
We will overestimate the size by a factor of 2.6!

### You need 16 times the sample size to estimate an interaction that is half the size as the main effect
- Lets do some simulations
- Assume that the true coefficients are 0. 
  - We don't really need a value since our main goal is looking at standard errors
- Assume that predictors are independent
- Let's use $x_1$ as our control/treatment variable and $x_2$ as our male/female variable. Our equation with group interactions then becomes

$$y = \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1\cdot x_2)$$

Now, let's write out equation for our 4 set of possible points $\{(y_{T,M}, x_T, x_M), (y_{C,M}, x_C, x_M), (y_{T,W}, x_T, x_W), (y_{C,W}, x_C, x_W)\}$. Truthfully, the $y$ variables should be averages at the specified $x$ coordinates, but added the bar to all the notation further down would be too much of a hassle. Further, $x_T, x_C$ are the two indicator (not necessarily 0/1) values $x_1$ can take, and $x_M, x_W$ are the two values $x_2$ can take. Of course, the $y$ values will have some sort of measurement error in them which requires some estimation procedure and not some simple algebra. The four equations are as follows:

\begin{align}
y_{T,M} &= \beta_1 x_T + \beta_2 x_M + \beta_3 (x_T\cdot x_M) \\
y_{C,M} &= \beta_1 x_C + \beta_2 x_M + \beta_3 (x_C\cdot x_M) \\
y_{T,W} &= \beta_1 x_T + \beta_2 x_W + \beta_3 (x_T\cdot x_W) \\
y_{C,W} &= \beta_1 x_C + \beta_2 x_W + \beta_3 (x_C\cdot x_W)
\end{align}

So how do we calculate the estimate for $(y_{T,M} - y_{C,M}) - (y_{T,W} - y_{C,W})$:
\begin{align}
(y_{T,M} - y_{C,M}) - (y_{T,W} - y_{C,W}) &= (\beta_1 x_T + \beta_2 x_M + \beta_3 (x_T\cdot x_M) - (\beta_1 x_C + \beta_2 x_M + \beta_3 (x_C\cdot x_M)) - (y_{T,W} - y_{C,W}) \\ 
&= \beta_1 (x_T - x_C) + \beta_3 (x_T\cdot x_M - x_C\cdot x_M) - (y_{T,W} - y_{C,W}) \\
&= \beta_1 (x_T - x_C) + \beta_3 (x_T\cdot x_M - x_C\cdot x_M) - (\beta_1 x_T + \beta_2 x_W + \beta_3 (x_T\cdot x_W) - \beta_1 x_C - \beta_2 x_W - \beta_3 (x_C\cdot x_W))\\
&= \beta_3 ((x_T\cdot x_M - x_C\cdot x_M) - (x_T\cdot x_W - x_C\cdot x_W))
\end{align}

Then, $\beta_3 = \frac{(y_{T,M} - y_{C,M}) - (y_{T,W} - y_{C,W})}{(x_T\cdot x_M - x_C\cdot x_M) - (x_T\cdot x_W - x_C\cdot x_W)}$. In most cases, $x_1$ (treatment) and $x_2$ (male/female) will take on values such that $(x_T\cdot x_M - x_C\cdot x_M) + (x_T\cdot x_W - x_C\cdot x_W)$ will be 1. If this is the case, then $$\beta_3 = (y_{T,M} - y_{C,M}) - (y_{T,W} - y_{C,W})$$
the difference in treated and control for male and treated and control for female.
\newline
\newline

Alright, we are almost there. Let's do $\beta_1$ and then move onto the standard errors of the parameters. 

\begin{align}
y_{T,M} - y_{C,M} &= (\beta_1 x_T + \beta_2 x_M + \beta_3 (x_T\cdot x_M)) - (\beta_1 x_C + \beta_2 x_M + \beta_3 (x_C\cdot x_M)) \\
&= \beta_1 (x_T - x_C) + \beta_3 (x_T\cdot x_M - x_C\cdot x_M) \\
&= \beta_1 (x_T - x_C) + \beta_3 x_M (x_T - x_C)
\end{align}

Thus, $$\beta_1 = \frac{y_{T,M} - y_{C,M}}{(x_T - x_C)} - \beta_3x_M$$

Enough equations, let's get estimating! Let's use 0.5 and -0.5 for coding $x_T$ and $x_C$, respectively. Same pattern for $x_M$ and $x_C$. 

```{r class.source = 'fold-show'}
n <- 1e3; sigma <- 10
y <- rnorm(n, 0, sigma)
x1 <- sample(c(0.5, -0.5), n, replace=T)
x2 <- sample(c(0.5, -0.5), n, replace=T)

fake <- data.frame(y,x1,x2)

fit_1 <- stan_glm(y ~ x1 + x2 + x1:x2, data=fake, refresh=0)
print(fit_1)
```


Again, ignore the estimates since they are pure noise. Our standard errors make sense. For $\beta_3$, we are performing addition and subtraction to 4 random variables (the mean of $y$ at any of the four specified $x$ combinations) with standard error $10/\sqrt{n}$. Thus, the standard error of $\beta_3$ is given by $4\sigma/\sqrt{n}=1.3$, which we calculated above.

Now, $\beta_1$, and by extension $\beta_2$ are a little trickier. We haven't really decided how to interpret them yet, and the issues lies within the coding of $x$. For the simulation above, we let $x$, for either covariates, take values of 0.5 and -0.5. Thus, the denominator for both $\beta_3$ and the first term of $\beta_1$ equal to 1. Thus $\beta_1$ becomes:

\begin{align}
\beta_1 &= y_{T,M} - y_{C,M} - x_M(\beta_3)\\
&= y_{T,M} - y_{C,M} - x_M(y_{T,M} - y_{C,M} - y_{T,W} + y_{C,W})\\
&= y_{T,M} - x_M y_{T,M} - y_{C,M} + x_M y_{C,M} + x_M y_{T,W} - x_M y_{C,W}\\ 
&= (1-x_M)y_{T,M} - (1-x_M)y_{C,M} + x_M(y_{T,W} - y_{C,W}) \\
&= (1-x_M)(y_{T,M} - y_{C,M}) + x_M(y_{T,W} - y_{C,W})
\end{align}

If we plug in $x_M=0.5$ for the coding above, we get $\beta_1 = (0.5)(y_{T,M} - y_{C,M}) + 0.5(y_{T,W} - y_{C,W})$ the average of the treatment effect observed in the male and treatment effect observed in the females! So, what is the variance of this estimate:

\begin{align}
V[\beta_1] &= V[0.5 (y_{T,M} - y_{C,M}) + V[0.5 (y_{T,W} - y_{C,W})]\\
&= 0.25 V[y_{T,M} - y_{C,M}] + 0.25 V[y_{T,W} - y_{C,W}] \\
&= 0.25\left(\frac{4\sigma^2}{n/4}\right)\\
&= \frac{4 \sigma^2}{n}
\end{align}

Taking the square root to get the standard error yields: $2\sigma/\sqrt{n}=0.3$

\newline

Now, what if we had coded our indicators as 1 and 0:

```{r class.source = 'fold-show'}
n <- 1e3; sigma <- 10
y <- rnorm(n, 0, sigma)
x1 <- sample(c(1, 0), n, replace=T)
x2 <- sample(c(1, 0), n, replace=T)

fake <- data.frame(y,x1,x2)

fit_1 <- stan_glm(y ~ x1 + x2 + x1:x2, data=fake, refresh=0)
print(fit_1)
```

Our standard errors increase by 50%! This is because $\beta_1$ is reporting a different quantity. I'll let you work through the math again, but if we use 1 and 0 for our indicators then $\beta_1$ becomes $$\beta_1 = y_{T,W} - y_{C,W}$$

The average treatment effect among females. Thus, the standard error is given by $\sqrt{4\sigma^2/n + 4\sigma^2/n} = \sigma\sqrt{8/n} \approx 0.9$.

\newline

Alright, last one. What if we used 1 and -1 as the indicators:

```{r class.source = 'fold-show'}
n <- 1e3; sigma <- 10
y <- rnorm(n, 0, sigma)
x1 <- sample(c(1, -1), n, replace=T)
x2 <- sample(c(1, -1), n, replace=T)

fake <- data.frame(y,x1,x2)

fit_1 <- stan_glm(y ~ x1 + x2 + x1:x2, data=fake, refresh=0)
print(fit_1)
```

Hmm, our standard errors went down by 50% from our original 0.5,-0.5 encoding of $x$. ROS gives a gives a great intuitive understanding: if $x$ is multiplied by a factor of 2, and $\beta x$ does not change, then $\beta$ must decrease by a factor of 2. Why does $\beta x$ not change? Because our $y$ generating process has not changed.  

For a more in-depth understanding of this particular scenario, I went into more detail in a [issue for AsKurz on github](https://github.com/ASKurz/Working-through-Regression-and-other-stories/issues/3).

# 16.5
### Design calculations afer the data have been collected

There is a study that was introduced in chapter 9.4 talking about how attractive parents are more likely to have girls. The study observed a significant effect with an estimate of 8% (female-male birth rate), with a standard error of 3%. Chapter nine does a good job diving into some reasons why this shouldn't be trusted, but here we'll talk about the implications of sign (type S) errors.

Moving forward, we are going to drop the 8% and only focus on the standard error of 3%. With a standard error of 3%, we would need to observe an effect of at least 6%  ($1.96\approx 2$), in either direction, to make the claim of stat sig.

Assume that the true difference is 0.2% and we keep our standard error of 3%, then conditioning on significance we'll observe an effect of:

```{r}
se <- 0.03
raw <- rnorm(1e6, mean=0.002, se)
signif <- raw > se*2
mean(raw[signif]) |> round(4)
```
So on average we'll overestimate our actual effect by 0.07/0.002 - almost 35 times! A very high type M error.

Now, onto the sign error. That is, given that we have observed significance and that the true effect is 0.2%, what is the probability that it is the correct sign.

```{r}
# Probability that sign is positive
pos <- 1 - pnorm(0.06, 0.002, 0.03)

# Probability that sign is negative
neg <- pnorm(-0.06, 0.002, 0.03)

(pos / (neg + pos)) |> round(2)
```
That is crazy! Only about a 60/40 chance that we will get the sign correct.

# 16.6
### Simulating randomized experiments

```{r class.source = 'fold-show'}
n <- 100
y_if_control <- rnorm(n, 60, 20)
y_if_treated <- y_if_control + 5 # Intervention adds 5 points

z <- sample(rep(c(0,1), n/2))
y <- ifelse(z == 1, y_if_treated, y_if_control)
fake <- data.frame(y, z)

diff <- mean(y[z == 1]) - mean(y[z == 0])
se <- sqrt(var(y[z==0])/(n/2) + var(y[z==1])/(n/2))
```

We can also run a regression to estimate the parameters we just calculated

```{r class.source = 'fold-show'}
fit <- stan_glm(y ~ z, data=fake, refresh=0)

print(fit)
cat('Mean diff from analytical solution', diff, 'se', se)
```

Our standard error is very noisy, around 4.3.

### Includng a pre-treatment predictor

```{r class.source = 'fold-show'}
fake$x <- rnorm(n, 50, 20)
fit_1b <- stan_glm(y ~ z + x, data=fake, refresh=0)
print(fit_1b)
```

Notice that our standard error for the coefficient of $z$, our a treatment effect, did not decrease. This is because $x$ is uncorrelated with $z$. 

To simulate a correlation we do follow an example of pre and post test scores for chapter 6. We say that there is some true ability plus some random noise that effects the pre-test score $x$. We then say that the post test score is again the true ability plus random noise and then some improvement factor. We will say that those that were treated will receive an additional improvement factor of 5.

```{r, class.source = 'fold-show'}
n <- 100
true_ability <- rnorm(n, 50, 16)
x <- true_ability + rnorm(n,0,12) # pre-test score
y_if_control <- true_ability + rnorm(n,0,12) + 10
y_if_treated <- y_if_control + 5

z <- sample(rep(c(1,0), n/2))
y <- ifelse(z == 1, y_if_treated, y_if_control)
fake_2 <- data.frame(x, y, z)

fit_2a <- stan_glm(y ~ z, data=fake_2, refresh=0)
print(fit_2a)

```

Standard error is still around 4.3. If we add $x$:

```{r class.source = 'fold-show'}
fit_2b <- stan_glm(y ~ z + x, data=fake_2, refresh=0)
print(fit_2b)
```


Our standard error for $z$ goes down by a third.

### Simulating an experiment with selection bias
The previous simulations assumed an RCT. Let's say now that students who performed poorly in the pre-test were more likely to receive the treatment. We'll say that $P(z_i = 1) = \text{logit}^{-1}(-(x_i-50)/20)$.

```{r class.source = 'fold-show', fig.align='center', fig.width=10, fig.height=6}
z <- rbinom(n, 1, invlogit(-(x-50)/20))
plot(x,z, ylab='P(z_i = 1)', pch=16)
seq(-5,120, length.out=1e4) %>%
  {lines(., invlogit(-(.-50)/20), lwd=3)}
```

We now run some simulations to show that if we do not condition on the pre-test, we end up with a horribly biased estimate:

```{r}
experiment <- function(n){
  true_ability <- rnorm(n, 50, 16)
  x <- true_ability + rnorm(n,0,12) # pre-test score
  y_if_control <- true_ability + rnorm(n,0,12) + 10
  y_if_treated <- y_if_control + 5
  
  z <- rbinom(n, 1, invlogit(-(x-50)/20))
  y <- ifelse(z == 1, y_if_treated, y_if_control)
  fake_3 <- data.frame(x, y, z)
  
  fit_3a <- stan_glm(y ~ z, data=fake_3, refresh=0)
  fit_3b <- stan_glm(y ~ z + x, data=fake_3, refresh=0)
  rbind(c(coef(fit_3a)['z'], se(fit_3a)['z']), c(coef(fit_3b)['z'], se(fit_3b)['z']))
}

n <- 100
n_loop <- 50
results <- array(NA, c(n_loop, 2, 2), 
                 dimnames <- list(1:n_loop, c('Simple', 'Adjusted'), 
                                  c('Estimate', 'SE')))
for (loop in 1:n_loop){
  results[loop,,] <- experiment(n)
}

apply(results, c(2,3), mean)
```

Wow, look at how the bias in sample for treated effect our outcome! It is very clear that we needed to adjust for the pre-test in order to get an accurate result.