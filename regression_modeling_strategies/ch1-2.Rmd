---
title: RMS Chapter 1-2
output: html_document
---
# Chapter 1
## 1.5
- Generalized degrees of freedom (GDF)
  - Index of "data dredging" or overfitting that has been done to the model

# Chapter 2
## 2.1 - 2.2
- Let $C(Y|X)$ be some property of the distribution of $Y$ given $X$. 
  - For example, in ordinary linear regression $C(Y|X) = E[Y|X]$
- For general linear regression, $C(Y|X) = g(X\beta)$.
  - For logistic regression that is $C(Y|X) = g(X\beta) = (1+\exp(-X\beta))^{-1}$.
  - Clearly in logistic regression $C(Y|X)$ is not linear in $X\beta$
- Define a transformation on $C(Y|X)$ called $h$ so $h(C(Y|X))$ is linear in $X\beta$
  - In other words, $h(u) = g^{-1}(u)$
- For logistic regression $h(u) = \text{logit}(u) = \log(u/(1-u))$ (log odds)
- Assume that $h(C(Y|X))=C(Y|X)$ for ease of notation since we will be using it quite a lot, are we are mostly concerned with linear relationships

## 2.3
For nominal predictors with 4 groups, model is sufficient with $C(Y|T)=X\beta=\beta_0 + \beta_1X_1 + \beta_2 X_2 + \beta_3X_3$. Notice that for a ANOVA, we only need to test $H_0: \beta_1 = \beta_2 = \beta_3 = 0$ and not include $beta_0$ since the other coefficients are just perturbations around $\beta_0$. In other words, if $\beta_1 = \beta_2 = \beta_3 = 0$ then $\beta_1 = \beta_2 = \beta_3 = \beta_0$ and all coefficients are equal.

### 2.3.2 Interactions
Consider the following model
$$C(Y|X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1X_2$$
where $x=(age,sex)$ and $X_2=I(x_2=\text{female})$. Refer to table 2.2 for some great examples of hypothesis testing and what the interaction actually means.

### 2.4.1
Basically never categorize your continuous data. If you do this based on values of $Y$, your type I and type II error are both elevated. If you really need cutpoints (unless otherwise theoretically motivated) on some predictor $X$, use a spline so it is at least smooth.

### 2.4.2
Consider the model:
$$C(Y|X_1) = \beta_0 + \beta_1X_1 + \beta_2X^2$$
If the model is truly linear $X_1$, $\beta_2$ will be zero. Use $H_0: \beta_2 = 0$ to test.


### 2.4.3
Suppose the x-axis is divided into intervals with endpoints at $a$, $b$, and $c$ called knots. The linear spline function is given by $$f(X) = \beta_0 + \beta_1X + \beta_2(X-a)_+ + \beta_3(X-b)_+ + \beta_4(X-c)_+$$

where $(u)_+ = u$ if $u > 0$, $0$ otherwise.



```{r}
n <- 30
x <- runif(n,0,100)
y <- ifelse(x <= 50, x*2 + rnorm(n,0,0.01), x*2 + (x-50)*4 + rnorm(n,0,0.01))
plot(x,y)
x1 <- ifelse(x - 50 <=0, 0, x - 50)
lm(y ~ x + x1)
```
Overall linearity in $X$ can be tested by testing $H_0: \beta_2 = \beta_3 = \beta_4$.

### 2.4.4-6
Cubic splines can also be used to fit highly curved functions. We can make smooth by forceing the first and second derivative to agree at the knots. A cubic spline with three knots is given by: 
$f(X) = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \beta_4(X-a)_+^3 + \beta_5(X-b)_+^3 + \beta_6(X-c)_+^3$ 

If a cubic spline has $k$  knots, then the function will require estimation of $k+3$ parameters.

We land on cubic since that allows us to have continutity on not only specifying that two segments must meet, but that their slope and second derivative as well must be the same.

One can also use restricted cubic splines by constraining the function to be linear in the tails. This is done since the function does not behave well in the tails of the parameter space. We also only have to estimate $k-1$ paramters. Function for this is given by
$f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_{k-1}X_{k-1}$

where $X_1=X$ and for $j=1,\dots,k-2$ $$X_{j+1} = (X-t_j)^3_+ - (X-t_{k-1})^3_+ \frac{t_k - t_j}{t_k - t_{k-1}} + (X-t_{k-1})^3_+ \frac{t_{k-1} - t_j}{t_k - t_{k-1}}$$
```{r}
library('Hmisc')

# knots are the knot locations
# The follow generates the data necessary to fit the restricted (natural) spline
x <- rcspline.eval(seq(0,1,.01), knots=seq(0.05, 0.95, length=5), inclx = TRUE)
xm <- x
xm[xm > 0.0106] <- NA

matplot(x[,1], xm, type='l', lty=1, lwd=3)
```
For choosing the position of knots, it has been shown that is matters more about the actual number of knots, rather than their positions. For a given number $k$, table 2.3 suggests the best location for them.

### 2.4.7
locally weighted least squares method (loess) is a non parameteric smoother. To perform loess at a given value $X=X$, fit a weighted linear regression around a region at $x$. This is usual done with an interval containing 2/3 of the data. Points are weighted such that data further from $x$ are given less weight then points closer (clearly). Like a moving average, only it is "moving linear regression"

### 2.5.2 (online only)
When to choose statistical modeling over ML?
- Uncertainty is inherent and the signal:noise ratio is small
- Additivity is the dominant way that predictors affect the outcome, or interactions are relatively small in number and can be pre-specified.
- Small sample size

### 2.7.1
Regression Assumptions
Assume $C(Y|X) = \beta_0 + \beta_1X_1 + \beta_2 X_2$ where $X_1$ is binary and $X_2$ is continuous.


- Fit the simple linear additive model and examine the residual plots.
  - Check that the spread of the residuals is consistent for boxplots stratified by binary covariate.
- Stratify sample by $X_1$ and quantile groups of $X_2$. Estimate $C(Y|X_1, X_2)$ for each stratum.
- For different levels of $X_1$, fit nonparametric smoothers (like loess) relating $X_2$ to $Y$.
 
Consider the following model: $\hat{C}(Y|X) = \hat{\beta_0} + \hat{\beta_1}X_1 + \hat{\beta_2}X_2 + \hat{\beta_3}X_2^\prime + \hat{\beta_4}X_2^{\prime\prime}$

Let $\hat{f}(X_2) = \hat{\beta_2}X_2 + \hat{\beta_3}X_2^\prime + \hat{\beta_4}X_2^{\prime\prime}$ such that $\hat{f}(X_2)$ is the spline-estimated transformation of $X_2$. If we plot $\hat{f}(X_2)$ vs. $X_2$ we will generally be able to shed light on how the effect of $X_2$ should be modeled. 

### 2.7.2
Complex interactions
For $X_1$ and $X_2$ continuous, let
$C(Y|X) = \beta_0 + \beta_1 X_1 + \beta_2 X_1^\prime + \beta_3 X_1^{\prime\prime} + \beta_4X_2 + \beta_5 X_2^\prime + \beta_6 X_2^{\prime \prime} + \beta_7 X_1 X_2 + \beta_8 X_1 X_2^\prime + \beta_9 X_1 X_2^{\prime\prime} + \beta_{10} X_1^\prime X_2 + \beta_{11} X_1^{\prime\prime} X_2$